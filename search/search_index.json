{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"simnetpy","text":"<p>Python Package for the creation and analysis of similarity networks.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install latest release from PyPi with pip</p> <pre><code>pip install simnetpy\n</code></pre>"},{"location":"#install-from-source","title":"Install from source","text":"<p>The source code for this project can be installed using <code>git</code> and <code>pip</code>. Clone repository</p> <pre><code>git clone https://github.com/amarnane/simnetpy.git\ncd simnetpy\npip install .\n</code></pre> <p>To remove the package simply use</p> <pre><code>pip uninstall simnetpy\n</code></pre>"},{"location":"#developer-mode","title":"Developer Mode","text":"<p>To install in developer mode (have changes in source code update without reinstallation) add <code>-e</code> flag</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#graph-tool","title":"Graph Tool","text":"<p>There is one dependency that cannot be installed through <code>pip</code> - <code>Graph-Tool</code>. This is a result of it's underlying <code>c++</code> dependencies. The simplest method for python users is to make use of a conda environment, install this package using the commands above and install <code>graph-tool</code> using <code>conda-forge</code></p> <pre><code>conda install -c conda-forge graph-tool\n</code></pre> <p>Note</p> <p>This will not work on Windows. Alternative (conda independent) solutions can be found on the Graph Tool Website</p>"},{"location":"#using-simnetpy","title":"Using <code>simnetpy</code>","text":"<pre><code>import simnet as sn\nimport numpy as np\n\n# create mixed guassian data with 100 nodes, 2 dimensions and 3 equally sized clusters.\nN = 100\nsizes=np.array([34,33,33])\nd = 2\ndataset = sn.datasets.mixed_multi_numeric(len(sizes), d, N, sizes=sizes)\n\n# calculate pairwise similarity\nS = sn.pairwise_sim(dataset.X, metric='euclidean', norm=True)\n\n# Create igraph Igraph from matrix\ngg = sn.network_from_sim_mat(S, method='knn', K=10)\n\n# print graph stats\nprint(gg.graph_stats())\n\n# true cluster quality\ncqual_ytrue = sn.clustering.cluster_quality(gg, dataset.y)\nprint(cqual_ytrue)\n\n# cluster\nylabels = sn.clustering.spectral_clustering(gg, laplacian='lrw')\n\n# cluster accuracy\ncacc = sn.clustering.cluster_accuracy(dataset.y, ylabels)\nprint(cacc)\n\n# predicted cluster quality\ncqual = sn.clustering.cluster_quality(gg, ylabels)\nprint(cqual)\n</code></pre>"},{"location":"using_simnetpy/","title":"Using SimNetPy","text":"In\u00a0[6]: Copied! <pre>%load_ext autoreload\n%autoreload 2\nimport simnetpy as sn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n</pre> %load_ext autoreload %autoreload 2 import simnetpy as sn import numpy as np import matplotlib.pyplot as plt from pprint import pprint <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> <p>Create some data using a mixed multi-guassian. Calculate similarity using the euclidean metric. Create a network by selecting the K-Nearest Neighbours (K=10) of each data point.</p> In\u00a0[19]: Copied! <pre>N = 100\nsizes=np.array([34,33,33])\nd = 2\ndataset = sn.datasets.mixed_multi_numeric(len(sizes), d, N, sizes=sizes)\n\n# calculate pairwise similarity\nS = sn.pairwise_sim(dataset.X, metric='euclidean', norm=True)\n\n# Create igraph Igraph from matrix\ngg = sn.network_from_sim_mat(S, method='knn', K=10)\n</pre> N = 100 sizes=np.array([34,33,33]) d = 2 dataset = sn.datasets.mixed_multi_numeric(len(sizes), d, N, sizes=sizes)  # calculate pairwise similarity S = sn.pairwise_sim(dataset.X, metric='euclidean', norm=True)  # Create igraph Igraph from matrix gg = sn.network_from_sim_mat(S, method='knn', K=10)   <p>We can look at the statistics of the graph:</p> <ul> <li>n - number of nodes</li> <li>E - number of edges</li> <li>Nc - number of connected components</li> <li>ncmax - number of nodes in the largest connected component.</li> <li>assortativity - degree mixing within the network</li> <li>avg_degree - Mean Degree of each vertex.</li> <li>max_degree - Max Degree of all vertices.</li> <li>median_degree - Median Degree of each vertex.</li> <li>avg_path_length - Average length of shortest path between all vertices.</li> <li>avglocalcc - Mean local clustering coefficient</li> <li>density - network density (portion of possible edges)</li> <li>globalcc - Global clustering coefficient</li> </ul> In\u00a0[20]: Copied! <pre># print graph stats\nprint('Graph Statistics:')\npprint(gg.graph_stats())\n# display(gg.graph_stats())\n</pre> # print graph stats print('Graph Statistics:') pprint(gg.graph_stats()) # display(gg.graph_stats()) <pre>Graph Statistics:\n{'E': 627,\n 'Nc': 1,\n 'assortivity': 0.13012665522902636,\n 'avg_degree': 12.54,\n 'avg_path_length': 4.084848484848485,\n 'avglocalcc': 0.6728595432775001,\n 'density': 0.12666666666666665,\n 'diameter': 10,\n 'globalcc': 0.6379537085744345,\n 'max_degree': 21,\n 'median_degree': 12.0,\n 'n': 100,\n 'ncmax': 100}\n</pre> <p>Cluster the graph using spectral clustering. The number of clusters is selected using the eigengap ratio heuristic.</p> In\u00a0[17]: Copied! <pre># cluster\nylabels = sn.clustering.spectral_clustering(gg, laplacian='lrw')\n</pre> # cluster ylabels = sn.clustering.spectral_clustering(gg, laplacian='lrw') <p>Evaluate the accuracy using:</p> <ul> <li>Adjusted Mutual Information</li> <li>Adjusted Rand Index</li> <li>Completeness</li> <li>Homogeneity</li> <li>V-Measure</li> </ul> <p>See sklearn.metrics for details.</p> In\u00a0[16]: Copied! <pre># cluster accuracy\ncacc = sn.clustering.cluster_accuracy(dataset.y, ylabels)\nprint('\\nPredicted Cluster Accuracy:')\npprint(cacc)\n</pre> # cluster accuracy cacc = sn.clustering.cluster_accuracy(dataset.y, ylabels) print('\\nPredicted Cluster Accuracy:') pprint(cacc) <pre>\nPredicted Cluster Accuracy:\n{'Npred': 3,\n 'Ntrue': 3,\n 'ami': 0.846420811615343,\n 'ari': 0.8829150927802232,\n 'complete': 0.8494134300611329,\n 'homo': 0.8491790791640881,\n 'vm': 0.8492962384461837}\n</pre> <p>Evaluate the cluster quality using:</p> <ul> <li>Conductance</li> <li>Density</li> <li>Modularity</li> <li>Separability</li> <li>Triad Participation Ratio</li> </ul> <p>See Defining and Evaluating Network Communities based on Ground-truth, Yang and Leskovec, 2012 for definitions.</p> In\u00a0[\u00a0]: Copied! <pre># predicted cluster quality\ncqual = sn.clustering.cluster_quality(gg, ylabels)\nprint('\\nPredicted Cluster Quality:')\npprint(cqual)\n</pre> # predicted cluster quality cqual = sn.clustering.cluster_quality(gg, ylabels) print('\\nPredicted Cluster Quality:') pprint(cqual) <pre>\nPredicted Cluster Quality:\n{'cc': 0.6257503295835909,\n 'cond': 0.06369122572576431,\n 'density': 0.36662521802464876,\n 'mod': 0.603128505047268,\n 'sep': 9.055860805860805,\n 'tpr': 1.0}\n</pre> <p>Thanks to the known ground truth cluster labels we can not only evaluate the quality of the predicted clusters but how well the network encodes the cluster quality.</p> In\u00a0[18]: Copied! <pre># true cluster quality\ncqual_ytrue = sn.clustering.cluster_quality(gg, dataset.y)\nprint('\\nGround Truth Cluster Quality:')\npprint(cqual_ytrue)\n</pre> # true cluster quality cqual_ytrue = sn.clustering.cluster_quality(gg, dataset.y) print('\\nGround Truth Cluster Quality:') pprint(cqual_ytrue) <pre>\nGround Truth Cluster Quality:\n{'cc': 0.625365265135371,\n 'cond': 0.0966136809881984,\n 'density': 0.3537953060011884,\n 'mod': 0.5701322404262138,\n 'sep': 5.4187917425622345,\n 'tpr': 0.9901960784313726}\n</pre> <p>Create some data. We create studentt and Gaussian distributed data with each of the cluster settings:</p> <ul> <li>Equal 3/10/30 - 3/10/30 equally sized clusters.</li> <li>Single Large - 10 clusters; 1 large cluster containing &gt;50% of nodes, 7 small clusters (1-5%) and 2 medium clusters (10%, 20%).</li> <li>Mixed Sizes - 10 clusters of mixed sizes; 3 larger clusters (20-30% of nodes), 2 medium clusters (5-10%) and 5 smaller clusters (1-5%)</li> </ul> In\u00a0[22]: Copied! <pre>N = 500\nd = 2\nstd = 1\ncluster_settings = sn.datasets.single_mod_cluster_problems(N)\ndistypes = ['guassian', 'studentt']\nrng = np.random.default_rng(seed=1871702)\n\ndata = {distype: {name: sn.datasets.mixed_multi_numeric(len(sizes), d, N, sizes=sizes, distype=distype, rng=rng) for name, sizes in cluster_settings.items()} for distype in distypes}\nprint(f'Distributions: {list(data.keys())}')\nprint(f'Cluster Problems: {list(data[\"guassian\"].keys())}')\n</pre> N = 500 d = 2 std = 1 cluster_settings = sn.datasets.single_mod_cluster_problems(N) distypes = ['guassian', 'studentt'] rng = np.random.default_rng(seed=1871702)  data = {distype: {name: sn.datasets.mixed_multi_numeric(len(sizes), d, N, sizes=sizes, distype=distype, rng=rng) for name, sizes in cluster_settings.items()} for distype in distypes} print(f'Distributions: {list(data.keys())}') print(f'Cluster Problems: {list(data[\"guassian\"].keys())}') <pre>Distributions: ['guassian', 'studentt']\nCluster Problems: ['equal_3', 'equal_10', 'equal_30', 'single_large', 'mixed_sizes']\n</pre> <p>Select the distribution and cluster type.</p> In\u00a0[9]: Copied! <pre>distype = 'guassian'\nclstr_name = 'mixed_sizes'\n# clstr_name = 'equal_10'\n\ndataset = data[distype][clstr_name]\nS = sn.pairwise_sim(dataset.X, metric='euclidean', norm=True)\n</pre> distype = 'guassian' clstr_name = 'mixed_sizes' # clstr_name = 'equal_10'  dataset = data[distype][clstr_name] S = sn.pairwise_sim(dataset.X, metric='euclidean', norm=True) <p>Plot the networks. Parameters have been chosen so the density of each network is roughly equal.</p> In\u00a0[24]: Copied! <pre>namedict = {'knn':'KNN','threshold':'Threshold','combined':'Combined', 'skewed_knn':'Linear Skewed KNN', 'log_skewed_knn':'Log Skewed KNN'}\n\n# Settings for graph creation. \nggs = [{'method':'knn', 'K':8} , {'method':'threshold', 't':0.02}, \n{'method':'combined', 'K':4, 't':0.0175}, {'method':'skewed_knn', 'K':9}, {'method':'log_skewed_knn', 'K':11}]\n\n# Create networks and scale by max degree\nmax_deg = []\ngraphs = {}\nfor gdict in ggs:\n    gg = sn.network_from_sim_mat(S, **gdict)\n    graphs[gdict['method']] = gg\n    # print(np.mean(gg.degree()))\n    max_deg.append(np.max(gg.degree()))\nmax_deg = np.array(max_deg)\nmax_deg = max_deg / max_deg.max()\n\n# plotting parameters\nalpha = 0.8\nedge_alpha=0.6\nmarkersize=8\nbase_size = 2.5\n\nsn.utils.set_science_style()\nfig, axs = plt.subplots(2,3,dpi=300, figsize=(9,6))\n\n# Data\nax = axs[0, 0]\nsn.plotting.plot_data_col_by_cluster(dataset.X, dataset.y, PCA=False, marker='.', markersize=base_size, ax=ax, alpha=alpha)\nax.set_title('Data')\n\n# Graphs\nfor i, (ax, gname) in enumerate(zip(axs.flatten()[1:], graphs.keys())):\n    g = graphs[gname]\n    prop = max_deg[i]\n    msize = markersize*prop\n    sn.graph.network_plot_col_by_cluster(g, dataset.X, dataset.y, PCA=False, ax=ax, markersize=msize, \n                    min_markersize=base_size, node_alpha=alpha, edge_alpha=edge_alpha, scale_marker=True)\n    ax.set_title(f'{namedict[gname]}')\n\n# f= f'network_comp-n{N}-{distype}-{clstr_name}_nodensity.png'\n# p = figfolder / f\n# sn.utils.save_mpl_figure(fig, savepath=p, svg=False, dpi=300)\nplt.show()\n</pre> namedict = {'knn':'KNN','threshold':'Threshold','combined':'Combined', 'skewed_knn':'Linear Skewed KNN', 'log_skewed_knn':'Log Skewed KNN'}  # Settings for graph creation.  ggs = [{'method':'knn', 'K':8} , {'method':'threshold', 't':0.02},  {'method':'combined', 'K':4, 't':0.0175}, {'method':'skewed_knn', 'K':9}, {'method':'log_skewed_knn', 'K':11}]  # Create networks and scale by max degree max_deg = [] graphs = {} for gdict in ggs:     gg = sn.network_from_sim_mat(S, **gdict)     graphs[gdict['method']] = gg     # print(np.mean(gg.degree()))     max_deg.append(np.max(gg.degree())) max_deg = np.array(max_deg) max_deg = max_deg / max_deg.max()  # plotting parameters alpha = 0.8 edge_alpha=0.6 markersize=8 base_size = 2.5  sn.utils.set_science_style() fig, axs = plt.subplots(2,3,dpi=300, figsize=(9,6))  # Data ax = axs[0, 0] sn.plotting.plot_data_col_by_cluster(dataset.X, dataset.y, PCA=False, marker='.', markersize=base_size, ax=ax, alpha=alpha) ax.set_title('Data')  # Graphs for i, (ax, gname) in enumerate(zip(axs.flatten()[1:], graphs.keys())):     g = graphs[gname]     prop = max_deg[i]     msize = markersize*prop     sn.graph.network_plot_col_by_cluster(g, dataset.X, dataset.y, PCA=False, ax=ax, markersize=msize,                      min_markersize=base_size, node_alpha=alpha, edge_alpha=edge_alpha, scale_marker=True)     ax.set_title(f'{namedict[gname]}')  # f= f'network_comp-n{N}-{distype}-{clstr_name}_nodensity.png' # p = figfolder / f # sn.utils.save_mpl_figure(fig, savepath=p, svg=False, dpi=300) plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"using_simnetpy/#using-simnetpy","title":"Using <code>SimNetPy</code>\u00b6","text":""},{"location":"using_simnetpy/#example-usage","title":"Example Usage\u00b6","text":"<p>Here we show some example usages of the <code>simnetpy</code> package.</p>"},{"location":"using_simnetpy/#generating-data","title":"Generating Data\u00b6","text":""},{"location":"using_simnetpy/#comparing-networks","title":"Comparing Networks\u00b6","text":""},{"location":"reference/clustering/","title":"Clustering","text":""},{"location":"reference/clustering/#clustering","title":"Clustering","text":""},{"location":"reference/clustering/#simnetpy.clustering.clustering","title":"<code>clustering</code>","text":""},{"location":"reference/clustering/#simnetpy.clustering.clustering.leiden_single_component_clustering","title":"<code>leiden_single_component_clustering(g, obj_func='modularity', beta=0.01, nsamples=15)</code>","text":"<p>cluster graph using leiden method. Uses event sampling to identify resolution parameter. resolution with maximum modularity is selected. Event sampling gives sequence of resolution  paramater that have more range of cluster numbers than a linear or logarithmic sequence. evenly covers one single cluster to all nodes are a separate cluster.  </p> <p>Parameters:</p> <ul> <li> <code>g</code>             (<code>Graph</code>)         \u2013          <p>graph to cluster</p> </li> <li> <code>obj_func</code>             (<code>str</code>, default:                 <code>'modularity'</code> )         \u2013          <p>function for leiden method. Defaults to 'modularity'. Other option CPM</p> </li> <li> <code>beta</code>             (<code>float</code>, default:                 <code>0.01</code> )         \u2013          <p>randomness in leiden algorithm (only used in refinement step). Defaults to 0.01.</p> </li> <li> <code>nsamples</code>             (<code>int</code>, default:                 <code>15</code> )         \u2013          <p>number of samples to use to approximate event curve in event sampling. Defaults to 50.                         Higher more accurate but incredibly slow in large networks with many different events.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: cluster labels</p> </li> </ul> Source code in <code>src/simnetpy/clustering/clustering.py</code> <pre><code>def leiden_single_component_clustering(g, obj_func='modularity', beta=0.01, nsamples=15):\n    \"\"\"cluster graph using leiden method. Uses event sampling to identify resolution parameter.\n    resolution with maximum modularity is selected. Event sampling gives sequence of resolution \n    paramater that have more range of cluster numbers than a linear or logarithmic sequence. evenly covers\n    one single cluster to all nodes are a separate cluster.  \n\n    Args:\n        g (ig.Graph): graph to cluster\n        obj_func (str, optional): function for leiden method. Defaults to 'modularity'. Other option CPM\n        beta (float, optional): randomness in leiden algorithm (only used in refinement step). Defaults to 0.01.\n        nsamples (int, optional): number of samples to use to approximate event curve in event sampling. Defaults to 50.\n                                    Higher more accurate but incredibly slow in large networks with many different events.\n\n    Returns:\n        np.ndarray: cluster labels\n    \"\"\"    \n    gammasq = resolution_event_samples(g,n=nsamples)\n    y_pred, gamma_max = find_max_mod_gamma_clstr(gammasq, g, obj_func, beta)\n    return y_pred\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.clustering.sbm_clustering","title":"<code>sbm_clustering(g, deg_corr=False, wait=10, nbreaks=2, beta=np.inf, mcmc_niter=10)</code>","text":"<p>Fit stochastic block model on g. </p> <p>Parameters:</p> <ul> <li> <code>g</code>             (<code>Graph or Graph</code>)         \u2013          <p>graph to cluster</p> </li> <li> <code>deg_corr</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>flag to use degree corrected model. Defaults to False.</p> </li> <li> <code>wait</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>number of steps required without record breaking event to end mcmc_equilibrate.                     Defaults to 10.</p> </li> <li> <code>nbreaks</code>             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>number of times <code>wait</code> steps need to happen consecutively. Defaults to 2.                     i.e. wait steps have to happen nbreaks times without a better state occuring.</p> </li> <li> <code>beta</code>             (<code>float (or np.inf)</code>, default:                 <code>inf</code> )         \u2013          <p>inverse temperature. controls types of proposal moves. beta=1 concentrates          on more plausible moves, beta=np.inf performs completely random moves. Defaults to 1.         exact detail: epsilon in equation 14) https://arxiv.org/pdf/2003.07070.pdf                     &amp; epsilon in equation 3) https://journals.aps.org/pre/pdf/10.1103/PhysRevE.89.012804</p> </li> <li> <code>mcmc_niter</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>number of gibbs sweeps to use in mcmc_merge_split. Defaults to 10.                     Higher values give better proposal moves i.e. quality of each swap improves but                      time spent on each step in monte carlo should be minimised.                     Discussion found in page 7 https://arxiv.org/pdf/2003.07070.pdf (parameter M used to estimate xhat)</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: cluster labels</p> </li> </ul> Source code in <code>src/simnetpy/clustering/clustering.py</code> <pre><code>def sbm_clustering(g, deg_corr=False, wait=10, nbreaks=2, beta=np.inf, mcmc_niter=10):\n    \"\"\"Fit stochastic block model on g. \n\n    Args:\n        g (gt.Graph or ig.Graph): graph to cluster\n        deg_corr (bool, optional): flag to use degree corrected model. Defaults to False.\n        wait (int, optional): number of steps required without record breaking event to end mcmc_equilibrate.\n                                Defaults to 10.\n        nbreaks (int, optional): number of times `wait` steps need to happen consecutively. Defaults to 2.\n                                i.e. wait steps have to happen nbreaks times without a better state occuring.\n        beta (float (or np.inf), optional): inverse temperature. controls types of proposal moves. beta=1 concentrates \n                    on more plausible moves, beta=np.inf performs completely random moves. Defaults to 1.\n                    exact detail: epsilon in equation 14) https://arxiv.org/pdf/2003.07070.pdf\n                                &amp; epsilon in equation 3) https://journals.aps.org/pre/pdf/10.1103/PhysRevE.89.012804\n        mcmc_niter (int, optional): number of gibbs sweeps to use in mcmc_merge_split. Defaults to 10.\n                                Higher values give better proposal moves i.e. quality of each swap improves but \n                                time spent on each step in monte carlo should be minimised.\n                                Discussion found in page 7 https://arxiv.org/pdf/2003.07070.pdf (parameter M used to estimate xhat)\n\n    Returns:\n        np.ndarray: cluster labels\n    \"\"\"\n    if gt == \"NOT INSTALLED\":\n        raise ImportError(\"The graph-tool module is not installed. SBM clustering unavailable. Use\\n\\t conda install \"+\\\n                \"-c conda-forge graph-tool\\nto install in a conda environment.\"+\\\n                \" See https://git.skewed.de/count0/graph-tool/-/wikis/installation-instructions \"+\\\n                \"for help.\")\n\n    if not isinstance(g, gt.Graph):\n        g = g.to_graph_tool()\n\n    state = gt.minimize_blockmodel_dl(g, state_args={'deg_corr':deg_corr})\n    gt.mcmc_equilibrate(state, wait=wait, nbreaks=nbreaks, mcmc_args=dict(niter=mcmc_niter, beta=beta))\n\n    y_pred = state.get_blocks().get_array() #state.b.a\n    y_pred = relabel(y_pred)\n    return y_pred\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.clustering.spectral_clustering","title":"<code>spectral_clustering(g, laplacian='lrw', cmetric='cosine', max_clusters=50, min_clusters=2)</code>","text":"<p>perform spectral clustering on graph on laplacian created from adjacency matrix. First Spectral decomp on laplacian.  Then uses eigengap to identify number of clusters K. Finally, clusters using K-means with user specified metric.</p> <p>cmet</p> <p>Parameters:</p> <ul> <li> <code>g</code>             (<code>Graph or ndarray</code>)         \u2013          <p>graph to cluster (also accepts adjacency matrices)</p> </li> <li> <code>laplacian</code>             (<code>str</code>, default:                 <code>'lrw'</code> )         \u2013          <p>Select laplacian from random walk <code>lrw</code>, symmetric <code>lsym</code>,         unnormalised <code>l</code> or adjacency <code>a</code>. Defaults to 'lrw'.</p> </li> <li> <code>cmetric</code>             (<code>str</code>, default:                 <code>'cosine'</code> )         \u2013          <p>metric to use in Kmeans cluster step. Any scipy pdist string or callable                      accepted. Defaults to 'cosine'.</p> </li> <li> <code>max_clusters</code>             (<code>int</code>, default:                 <code>50</code> )         \u2013          <p>max number of clusters to accept. Defaults to 50.</p> </li> <li> <code>min_clusters</code>             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>min number of clusters to accept. Defaults to 2 (min=1 may not work).</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: cluster labels</p> </li> </ul> Source code in <code>src/simnetpy/clustering/clustering.py</code> <pre><code>def spectral_clustering(g, laplacian='lrw', cmetric='cosine', max_clusters=50, min_clusters=2):\n    \"\"\"perform spectral clustering on graph on laplacian created from adjacency matrix. First Spectral decomp on laplacian. \n    Then uses eigengap to identify number of clusters K. Finally, clusters using K-means with user specified metric.\n\n    cmet\n\n    Args:\n        g (ig.Graph or np.ndarray): graph to cluster (also accepts adjacency matrices)\n        laplacian (str, optional): Select laplacian from random walk `lrw`, symmetric `lsym`,\n                    unnormalised `l` or adjacency `a`. Defaults to 'lrw'.\n        cmetric (str, optional): metric to use in Kmeans cluster step. Any scipy pdist string or callable \n                                accepted. Defaults to 'cosine'.\n        max_clusters (int, optional): max number of clusters to accept. Defaults to 50.\n        min_clusters (int, optional): min number of clusters to accept. Defaults to 2 (min=1 may not work).\n\n    Returns:\n        np.ndarray: cluster labels\n    \"\"\"\n    if not isinstance(g, np.ndarray):\n        A = g.get_adjacency_sparse().toarray()\n    else:\n        A = g\n\n    spect = Spectral(laplacian_type=laplacian, custom_dist=cmetric, min_clusters= min_clusters, max_clusters=max_clusters)\n    y_pred = spect.predict_from_adj(A)\n    return y_pred\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.event_sampling","title":"<code>event_sampling</code>","text":""},{"location":"reference/clustering/#simnetpy.clustering.event_sampling.resolution_event_samples","title":"<code>resolution_event_samples(g, n=15, plot=False, n_to_approx_beta=50, return_dict=False)</code>","text":"<p>Function to identify a good sequence of resolutions for resolution parameter in leiden clustering. Samples are evenly spaced over the difference levels of hierarchy. From all nodes in single cluster to each node in individual cluster. Implementation of the method described in paper doi: 10.1038/s41598-018-21352-7</p> <p>Parameters:</p> <ul> <li> <code>g</code>             (<code>Graph</code>)         \u2013          <p>igraph network to be clustered</p> </li> <li> <code>n</code>             (<code>int</code>, default:                 <code>15</code> )         \u2013          <p>length of sequence of resolutions to find. Defaults to 15.</p> </li> <li> <code>plot</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Flag to plot beta-gamma event sample curve (as in the paper). Defaults to False.</p> </li> <li> <code>n_to_approx_beta</code>             (<code>int</code>, default:                 <code>50</code> )         \u2013          <p>In large networks, the number of events can be very large. Finding all beta events can take a long time.                      This is the number of samples used to appoximate the event curve. Defaults to 50.                     Note: 50 other samples are used in the approximation although these are kept fixed so total is n_to_approx_beta+50.</p> </li> <li> <code>return_dict</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Flag to return beta samples as well as gamma samples. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_type_</code>        \u2013          <p>description</p> </li> </ul> Source code in <code>src/simnetpy/clustering/event_sampling.py</code> <pre><code>def resolution_event_samples(g, n=15, plot=False, n_to_approx_beta=50, return_dict=False):\n    \"\"\"Function to identify a good sequence of resolutions for resolution parameter in leiden clustering.\n    Samples are evenly spaced over the difference levels of hierarchy. From all nodes in single cluster to each node in individual cluster.\n    Implementation of the method described in paper doi: 10.1038/s41598-018-21352-7\n\n    Args:\n        g (ig.Graph): igraph network to be clustered\n        n (int, optional): length of sequence of resolutions to find. Defaults to 15.\n        plot (bool, optional): Flag to plot beta-gamma event sample curve (as in the paper). Defaults to False.\n        n_to_approx_beta (int, optional): In large networks, the number of events can be very large. Finding all beta events can take a long time. \n                                This is the number of samples used to appoximate the event curve. Defaults to 50.\n                                Note: 50 other samples are used in the approximation although these are kept fixed so total is n_to_approx_beta+50.\n\n        return_dict (bool, optional): Flag to return beta samples as well as gamma samples. Defaults to False.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n\n    A = g.get_adjacency_sparse()\n    A = A.toarray()\n    np.fill_diagonal(A,0) # don't want self loops\n\n    D = A.sum(axis=0)\n\n\n    P = np.outer(D,D)/(D.sum())\n    np.fill_diagonal(P, 0)\n\n    P = squareform(P)\n    A = squareform(A)\n\n    Q = A/P\n    ymax = Q.max()\n    ymin = find_ymin(g)\n\n    Bmin = betaf(ymin, A, P)\n    Bmax = betaf(ymax, A, P)\n\n    events, bevents = sample_events(Q, A, P, n_to_approx_beta=n_to_approx_beta)\n\n    beta_samples = np.linspace(Bmin,Bmax, num=n, endpoint=False)\n    samples = np.array([gammaf(B, A, P, events, bevents) for B in beta_samples])\n\n    if plot:\n        yseq = np.logspace(np.log(ymin),np.log10(ymax), num=n_to_approx_beta)\n        bseq = np.array([betaf(yy,A,P) for yy in yseq])\n\n        fig = plot_beta_curve(samples, beta_samples, yseq, bseq)\n\n    if return_dict:\n        {'gamma_samples': samples, 'beta_samples': beta_samples}\n    else:\n        return samples\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.event_sampling.sample_events","title":"<code>sample_events(Q, A, P, n_to_approx_beta=100)</code>","text":"<p>Function to approximate beta event curve. Speeds computation for large networks with a large number of events.</p> <p>Parameters:</p> <ul> <li> <code>Q</code>             (<code>ndarray</code>)         \u2013          <p>A/P matrix pre-calculated and in squareform (i.e. just triu entries)</p> </li> <li> <code>A</code>             (<code>ndarray</code>)         \u2013          <p>Adjacency matrix</p> </li> <li> <code>P</code>             (<code>ndarray</code>)         \u2013          <p>expected adjacency matrix      (configuration model assumed - k_i*k_j/2m where k_i is degree of ith node and m is number of edges in network)</p> </li> <li> <code>n_to_approx_beta</code>             (<code>int</code>, default:                 <code>100</code> )         \u2013          <p>This is the number of samples used to appoximate the event curve. Defaults to 50.                     Note: 50 other samples are used in the approximation although these are kept fixed                      so total is n_to_approx_beta+50 Defaults to 100.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>events, bevents: sequence of precalculated gamma and beta events.</p> </li> </ul> Source code in <code>src/simnetpy/clustering/event_sampling.py</code> <pre><code>def sample_events(Q, A, P, n_to_approx_beta=100):\n    \"\"\"\n    Function to approximate beta event curve. Speeds computation for large networks with a large number of events.\n\n    Args:\n        Q (np.ndarray): A/P matrix pre-calculated and in squareform (i.e. just triu entries)\n        A (np.ndarray): Adjacency matrix\n        P (np.ndarray): expected adjacency matrix \n                (configuration model assumed - k_i*k_j/2m where k_i is degree of ith node and m is number of edges in network)\n        n_to_approx_beta (int, optional): This is the number of samples used to appoximate the event curve. Defaults to 50.\n                                Note: 50 other samples are used in the approximation although these are kept fixed \n                                so total is n_to_approx_beta+50 Defaults to 100.\n\n    Returns:\n        events, bevents: sequence of precalculated gamma and beta events.\n    \"\"\"\n    # Find unique events\n    events = np.unique(Q)[1:] # ignore 0 \n\n    # Event curves are very skewed with large number of events occuring in highest 50%\n    # This is also the area of least interest as most communities are single or isolated nodes.\n    # we take look each percentile between 50 and 100 i.e. 51%, 52%, ...\n    qq = np.linspace(0.5,1, num=51)\n    upper_vals = np.array([np.quantile(events,q) for q in qq])\n\n    # we only want to approximate the lower events with nsample points\n    mid = np.median(events)\n    events = events[events &lt; mid]\n\n\n    step = events.shape[0] // n_to_approx_beta\n    # if there are less than nsample events step=0 which raises error.\n    # use stepsize of 1 in this case. equivalent to n_to_approx_beta = events.shape[0]\n    if not step:\n        step = 1\n    # sample an event after every step events (step dictated by n_to_approx_beta)\n    events = events[::step]\n\n    # calculate beta value for each event\n    events = np.concatenate((events, upper_vals))\n    bevents = np.array([betaf(e, A, P) for e in events])\n\n    return events, bevents\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.quality","title":"<code>quality</code>","text":""},{"location":"reference/clustering/#simnetpy.clustering.quality.cluster_quality","title":"<code>cluster_quality(g, y)</code>","text":"<p>Return stats describing cluster quality  - conductance - modularity - triad participation ratio and \"community goodness\" - separability - density - clustering coefficient note: ideas of cluster quality and community goodness  taken from https://dl.acm.org/doi/abs/10.1145/2350190.2350193</p> <p>Parameters:</p> <ul> <li> <code>g</code>             (<code>_type_</code>)         \u2013          <p>description</p> </li> <li> <code>y</code>             (<code>_type_</code>)         \u2013          <p>description</p> </li> </ul> Source code in <code>src/simnetpy/clustering/quality.py</code> <pre><code>def cluster_quality(g, y):\n    \"\"\"Return stats describing cluster quality \n    - conductance\n    - modularity\n    - triad participation ratio\n    and \"community goodness\"\n    - separability\n    - density\n    - clustering coefficient\n    note: ideas of cluster quality and community goodness \n    taken from https://dl.acm.org/doi/abs/10.1145/2350190.2350193\n\n    Args:\n        g (_type_): _description_\n        y (_type_): _description_\n    \"\"\"\n    tpr = avg_triangle_participation_ratio(g, y)\n    mod = g.modularity(y)\n    cond = avg_conductance(g, y)\n    dens = avg_density(g, y)\n    sep = avg_separability(g, y)\n    cc = avg_clustering(g, y)\n\n    ddict= {'mod':mod, 'cond':cond, 'tpr':tpr,\n            'sep':sep, 'density':dens, 'cc':cc}\n    return ddict\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.quality.triangle_participation_ratio","title":"<code>triangle_participation_ratio(g)</code>","text":"<p>calculate triad particpant ratio for a graph. TPR is the fraction of nodes in g that belong in a triad.</p> <p>Parameters:</p> <ul> <li> <code>g</code>             (<code>Graph</code>)         \u2013          <p>graph to find </p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>        \u2013          <p>fraction of nodes in a triad</p> </li> </ul> Source code in <code>src/simnetpy/clustering/quality.py</code> <pre><code>def triangle_participation_ratio(g):\n    \"\"\"calculate triad particpant ratio for a graph.\n    TPR is the fraction of nodes in g that belong in a triad.\n\n    Args:\n        g (ig.Graph): graph to find \n\n    Returns:\n        float: fraction of nodes in a triad\n    \"\"\"\n    vintriad = 0\n    for v in g.vs:\n        v_nbrs = g.neighbors(v.index)\n        vs = set(v_nbrs) - {v.index}\n        gen_degree = Counter(len(vs &amp; (set(g.neighbors(w)) - {w})) for w in vs)\n        ntriangle = sum(k*val for k, val in gen_degree.items())\n        if ntriangle:\n            vintriad +=1\n    tp_ratio = vintriad/g.vcount()\n    return tp_ratio\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.spectral","title":"<code>spectral</code>","text":"<p>Extension of spectral cluster class developed in https://pypi.org/project/spectralcluster/ Allow the passing of adjacency matrices. Adjusted laplacian and eigengap parameters to accept strings. Removed a lot of functionality relating to constraint options and refinement preprocessing.</p>"},{"location":"reference/clustering/#simnetpy.clustering.spectral.Spectral","title":"<code>Spectral</code>","text":"<p>             Bases: <code>SpectralClusterer</code></p> Source code in <code>src/simnetpy/clustering/spectral.py</code> <pre><code>class Spectral(sc.SpectralClusterer):\n    def __init__(self, \n                min_clusters=2,\n                max_clusters=10, \n                laplacian_type='lrw', \n                stop_eigenvalue=1e-2, \n                custom_dist=\"cosine\", \n                eigengap_type='ratio', **kwds):\n        ltype = LTYPES[laplacian_type.lower()] # lookup laplacian type class, A/L/Lrw/Lsym (all get lower cased)\n        egaptype = EIGENGAPCOMP[eigengap_type] # lookup eigengap comp type\n\n        super().__init__(min_clusters=min_clusters,\n                max_clusters=max_clusters, \n                laplacian_type=ltype, \n                stop_eigenvalue=stop_eigenvalue, \n                custom_dist=custom_dist, \n                eigengap_type=egaptype, **kwds)\n\n    def predict_from_adj(self, A, laplacian_type=None):\n        if laplacian_type is not None:\n            self.laplacian_type = LTYPES[laplacian_type.lower()]\n\n        n_samples = A.shape[0]\n\n        constraint_matrix=None # ignore constraint for now. not clear what use is.\n        eigenvectors, n_clusters, _ = self._compute_eigenvectors_ncluster(\n            A, constraint_matrix)\n\n        if self.min_clusters is not None:\n            n_clusters = max(n_clusters, self.min_clusters)\n\n        # Get spectral embeddings.\n        spectral_embeddings = eigenvectors[:, :n_clusters]\n\n        if self.row_wise_renorm:\n            # Perform row wise re-normalization.\n            rows_norm = np.linalg.norm(spectral_embeddings, axis=1, ord=2)\n            spectral_embeddings = spectral_embeddings / np.reshape(\n                rows_norm, (n_samples, 1))\n\n        # Run clustering algorithm on spectral embeddings. This defaults\n        # to customized K-means.\n        # kmeans is raising future warning. rather than rewrite entire class. I added a simple warning filter.\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            labels = self.post_eigen_cluster_function(\n                spectral_embeddings=spectral_embeddings,\n                n_clusters=n_clusters,\n                custom_dist=self.custom_dist,\n                max_iter=self.max_iter)\n        return labels\n\n\n    def predict_from_aff(self, X=None, S=None, metric='euclidean', norm=True):\n        \"\"\"Perform spectral clustering on an affinity matrix. \n        Note affinity matrix should have form where larger values indicate higher similarity i.e. opposite of distance \n        Args:\n            X:      embedding/feature matrix n x d np.ndarray\n            S:    pairwise affinity (Similarity) matrix n x n np.ndarray. \n            metric: metric to be used in pairwise distance\n            norm:   wether to normalise Aff to be 0 mean 1 std\n        Returns:\n            labels: numpy array of shape (n_samples,)\n        \"\"\"\n        self.laplacian_type = LTYPES['a'] # set laplacian type as affinity\n        if (X is None) and (S is None):\n            raise ValueError('One of X or S must be specified. Note if both are specified then S is used.')\n\n        if S is None:\n            affinity = self.affinity_matrix(X, metric=metric, norm=norm)\n        else:\n            affinity = S\n\n        num_embeddings = affinity.shape[0]\n\n        # # Compute affinity matrix.\n        # affinity = self.affinity_function(embeddings)\n        constraint_matrix = None\n\n        eigenvectors, n_clusters, _ = self._compute_eigenvectors_ncluster(\n                affinity, constraint_matrix)\n\n        if self.min_clusters is not None:\n            n_clusters = max(n_clusters, self.min_clusters)\n\n        # Get spectral embeddings.\n        spectral_embeddings = eigenvectors[:, :n_clusters]\n\n        if self.row_wise_renorm:\n            # Perform row wise re-normalization.\n            rows_norm = np.linalg.norm(spectral_embeddings, axis=1, ord=2)\n            spectral_embeddings = spectral_embeddings / np.reshape(\n                rows_norm, (num_embeddings, 1))\n\n        # Run clustering algorithm on spectral embeddings. This defaults\n        # to a customized implementation of K-means. However, the implementation was created on old sklearn version.\n        # -&gt; kmeans raises a Future Warning for n_init parameter.  Rather than rewrite entire class. I added a simple warning filter.\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            labels = self.post_eigen_cluster_function(\n                spectral_embeddings=spectral_embeddings,\n                n_clusters=n_clusters,\n                custom_dist=self.custom_dist,\n                max_iter=self.max_iter)\n        return labels\n\n    @staticmethod\n    def affinity_matrix(X, metric='euclidean', norm=True):\n        S = pairwise_sim(X, method=metric, norm=norm)\n        S = -S  # want higher values to be more similar\n        return S\n</code></pre>"},{"location":"reference/clustering/#simnetpy.clustering.spectral.Spectral.predict_from_aff","title":"<code>predict_from_aff(X=None, S=None, metric='euclidean', norm=True)</code>","text":"<p>Perform spectral clustering on an affinity matrix.  Note affinity matrix should have form where larger values indicate higher similarity i.e. opposite of distance  Args:     X:      embedding/feature matrix n x d np.ndarray     S:    pairwise affinity (Similarity) matrix n x n np.ndarray.      metric: metric to be used in pairwise distance     norm:   wether to normalise Aff to be 0 mean 1 std Returns:     labels: numpy array of shape (n_samples,)</p> Source code in <code>src/simnetpy/clustering/spectral.py</code> <pre><code>def predict_from_aff(self, X=None, S=None, metric='euclidean', norm=True):\n    \"\"\"Perform spectral clustering on an affinity matrix. \n    Note affinity matrix should have form where larger values indicate higher similarity i.e. opposite of distance \n    Args:\n        X:      embedding/feature matrix n x d np.ndarray\n        S:    pairwise affinity (Similarity) matrix n x n np.ndarray. \n        metric: metric to be used in pairwise distance\n        norm:   wether to normalise Aff to be 0 mean 1 std\n    Returns:\n        labels: numpy array of shape (n_samples,)\n    \"\"\"\n    self.laplacian_type = LTYPES['a'] # set laplacian type as affinity\n    if (X is None) and (S is None):\n        raise ValueError('One of X or S must be specified. Note if both are specified then S is used.')\n\n    if S is None:\n        affinity = self.affinity_matrix(X, metric=metric, norm=norm)\n    else:\n        affinity = S\n\n    num_embeddings = affinity.shape[0]\n\n    # # Compute affinity matrix.\n    # affinity = self.affinity_function(embeddings)\n    constraint_matrix = None\n\n    eigenvectors, n_clusters, _ = self._compute_eigenvectors_ncluster(\n            affinity, constraint_matrix)\n\n    if self.min_clusters is not None:\n        n_clusters = max(n_clusters, self.min_clusters)\n\n    # Get spectral embeddings.\n    spectral_embeddings = eigenvectors[:, :n_clusters]\n\n    if self.row_wise_renorm:\n        # Perform row wise re-normalization.\n        rows_norm = np.linalg.norm(spectral_embeddings, axis=1, ord=2)\n        spectral_embeddings = spectral_embeddings / np.reshape(\n            rows_norm, (num_embeddings, 1))\n\n    # Run clustering algorithm on spectral embeddings. This defaults\n    # to a customized implementation of K-means. However, the implementation was created on old sklearn version.\n    # -&gt; kmeans raises a Future Warning for n_init parameter.  Rather than rewrite entire class. I added a simple warning filter.\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        labels = self.post_eigen_cluster_function(\n            spectral_embeddings=spectral_embeddings,\n            n_clusters=n_clusters,\n            custom_dist=self.custom_dist,\n            max_iter=self.max_iter)\n    return labels\n</code></pre>"},{"location":"reference/datasets/","title":"Datasets","text":""},{"location":"reference/datasets/#datasets","title":"Datasets","text":""},{"location":"reference/datasets/#simnetpy.datasets.cluster_centers","title":"<code>cluster_centers(n, d, lower=1, higher=2, rng=None, init=None)</code>","text":"<p>Sample n point in d-dimensional space. Points will be between (lower, ~2*higher) distance from all other points. Initial center will be (0,0, ..,0) unless otherwise specified. Sampling done sequentially with next center proposed, rejected if too close to all others and resampled until accepted. If sampling large number of points &amp; time taken is large increase size of higher.</p> <p>Note: 2higher is not actual distance upper bound. higher controls size of box around previous center that we sample a proposal point. Each center sampled from box with sides of size 2higher. Args:     n (int): number of points to generate     d (int): number of dimensions     lower (float, optional): Lower bound of distances to accept. All points will be                         at least lower away from each other. Defaults to 1.     higher (float, optional): Size of box around previous center to sample from. Defaults to 2.     rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.     init (np.ndarray, optional): Location of first sample. Defaults to None. Note points are shuffled     but if none at least one will be the origin (0,0,...,0).</p> <p>Returns:</p> <ul> <li> <code>list</code>        \u2013          <p>n randomly sampled points in d-dimensional space all at least lower away from each other.</p> </li> </ul> Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def cluster_centers(n, d, lower=1, higher=2, rng=None, init=None):\n    \"\"\"Sample n point in d-dimensional space. Points will be between\n    (lower, ~2*higher) distance from all other points. Initial center will be (0,0, ..,0)\n    unless otherwise specified. Sampling done sequentially with next center proposed,\n    rejected if too close to all others and resampled until accepted.\n    If sampling large number of points &amp; time taken is large increase size of higher.\n\n    Note: 2*higher is not actual distance upper bound. higher controls size of box around previous center\n    that we sample a proposal point. Each center sampled from box with sides of size 2*higher.\n    Args:\n        n (int): number of points to generate\n        d (int): number of dimensions\n        lower (float, optional): Lower bound of distances to accept. All points will be\n                            at least lower away from each other. Defaults to 1.\n        higher (float, optional): Size of box around previous center to sample from. Defaults to 2.\n        rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.\n        init (np.ndarray, optional): Location of first sample. Defaults to None. Note points are shuffled\n        but if none at least one will be the origin (0,0,...,0).\n\n    Returns:\n        list: n randomly sampled points in d-dimensional space all at least lower away from each other.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if init is None:\n        init = np.zeros(d)\n    centers = []\n    x = init + uniform_sampler(d, std=higher, rng=rng)\n    centers.append(x)\n\n    while len(centers) &lt; n:\n        # sample point in random direction\n        x = uniform_sampler(d, std=higher, rng=rng)\n\n        # find random center and move away in direction x\n        i = rng.integers(0, len(centers))\n        x = centers[i] + x\n\n        carray = np.array(centers)\n        dist = np.linalg.norm(carray - x, axis=1, ord=2)\n\n        if np.all(dist &gt; lower):\n            centers.append(x)\n    rng.shuffle(centers)  # shuffle so first cluster is not necessarily close to init\n    return centers\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.mixed_categorical_clusters","title":"<code>mixed_categorical_clusters(N, d, nclusters=3, sizes='equal', alpha=5, beta=1, nlevels=5, return_skew_factors=True, rng=None)</code>","text":"<p>alpha and beta control shape of skew factor distribution higher max(abs(a,b))&gt;1 -&gt; less flat more peaked distribution a skewed below 0.5 a&gt;b -&gt; skewed above 0.5 so 5,1 means on average skew passed to ordinal feature generator will be centered on a/a+b~0.83 and 1,5 means average skew will be ~0.16 lower average skew means noisier data and harder clustering problem Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def mixed_categorical_clusters(\n    N,\n    d,\n    nclusters=3,\n    sizes=\"equal\",\n    alpha=5,\n    beta=1,\n    nlevels=5,\n    return_skew_factors=True,\n    rng=None,\n):\n    \"\"\"alpha and beta control shape of skew factor distribution\n    higher max(abs(a,b))&gt;1 -&gt; less flat more peaked distribution\n    a&lt;b -&gt; skewed below 0.5\n    a&gt;b -&gt; skewed above 0.5\n    so 5,1 means on average skew passed to ordinal feature generator will be centered on a/a+b~0.83\n    and 1,5 means average skew will be ~0.16\n    lower average skew means noisier data and harder clustering problem\n\n    \"\"\"\n\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if isinstance(sizes, str):\n        assert sizes.lower() in [\n            \"equal\",\n            \"random\",\n            \"roughly_equal\",\n        ], \"if specifying method sizes must be one of [equal, random, roughly_equal]\"\n        sizes = split_data_into_clusters(N, nclusters, method=sizes)\n    elif sizes is None:\n        sizes = split_data_into_clusters(N, nclusters, method=\"equal\")\n\n    assert sizes.shape[0] == nclusters, \"nclusters and sizes must match\"\n    assert sizes.sum() == N, f\"sizes must add up to N {sizes.sum()} != {N}\"\n\n    # N = sizes.sum()\n\n    # generate skew distribution &amp; sample\n    rv = stats.beta(a=alpha, b=beta)\n    skew_factors = rv.rvs(size=d, random_state=rng)\n\n    # generate features\n    X = np.zeros((N, d))\n    for i in range(d):\n        si = skew_factors[i]\n        if si &gt;= 0.5 and si &lt;= 1:\n            skew = (\n                2 * ((nlevels - 1) / nlevels) * si + (2 - nlevels) / nlevels\n            )  # map skew factor from [0.5, 1] to [1/nlevels, 1] i.e. si=0.5, skew=1/nlevels\n            X[:, i] = mixed_categorical_cluster_feature(\n                sizes, nlevels=nlevels, min_largest_cat=skew, rng=rng\n            )\n        elif si &gt;= 0 and si &lt; 0.5:\n            skew = (\n                2 * ((1 - nlevels) / nlevels) * si + 1\n            )  # map skew factor from [0.5, 0] to [1/nlevels, 1] i.e si=0 -&gt; skew = 1.0\n            X[:, i] = single_categorical_feature(\n                N, nlevels=nlevels, min_largest_cat=skew, rng=rng\n            )\n        else:\n            raise ValueError(\"Error beta distribution ill defined\")\n\n    # generate labels\n    y = np.zeros(N)\n    total = 0\n    for i, size in enumerate(sizes):\n        y[total : total + size] = i\n        total += size\n\n    dataset = Bunch(y=y, X=X)\n    if return_skew_factors:\n        dataset[\"skew_factors\"] = skew_factors\n\n    return dataset\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.multivariate_guassian","title":"<code>multivariate_guassian(N, center, std=1, rng=None)</code>","text":"<p>Sample N point from a multivariate guassian with mean at center and Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred from user passed center.</p> <p>Parameters:</p> <ul> <li> <code>N</code>             (<code>int</code>)         \u2013          <p>number of points to sample</p> </li> <li> <code>center</code>             (<code>ndarray</code>)         \u2013          <p>d-dimensional point</p> </li> <li> <code>std</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Standard deviation along any axis. Covariance is <code>std</code>*Identity Matrix. Defaults to 1.</p> </li> <li> <code>rng</code>             (<code>default_rng</code>, default:                 <code>None</code> )         \u2013          <p>user seeded random number generator. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: N samples from multi-dimensional guassian centered at <code>center</code>. (N x d) matrix</p> </li> </ul> Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def multivariate_guassian(N, center, std=1, rng=None):\n    \"\"\"Sample N point from a multivariate guassian with mean at center and\n    Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred\n    from user passed center.\n\n    Args:\n        N (int): number of points to sample\n        center (np.ndarray): d-dimensional point\n        std (float, optional): Standard deviation along any axis. Covariance is `std`*Identity Matrix. Defaults to 1.\n        rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.\n\n    Returns:\n        np.ndarray: N samples from multi-dimensional guassian centered at `center`. (N x d) matrix\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if not isinstance(std, np.ndarray):\n        d = center.shape[0]\n        COV = std * np.eye(d)\n    else:\n        COV = std\n\n    X = rng.multivariate_normal(center, COV, size=N)\n    return X\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.multivariate_t","title":"<code>multivariate_t(N, center, std=1, df=1, rng=None)</code>","text":"<p>Sample N point from a multivariate guassian with mean at center and Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred from user passed center.</p> <p>Parameters:</p> <ul> <li> <code>N</code>             (<code>int</code>)         \u2013          <p>number of points to sample</p> </li> <li> <code>center</code>             (<code>ndarray</code>)         \u2013          <p>d-dimensional point</p> </li> <li> <code>std</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Standard deviation along any axis. Covariance is <code>std</code>*Identity Matrix. Defaults to 1.                     Also accepts numpy covariance matrix</p> </li> <li> <code>df</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Degrees of freedom of the distribution. Defaults to 1. If np.inf results are multivariate normal.</p> </li> <li> <code>rng</code>             (<code>default_rng</code>, default:                 <code>None</code> )         \u2013          <p>user seeded random number generator. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: N samples from multi-dimensional guassian centered at <code>center</code>. (N x d) matrix</p> </li> </ul> Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def multivariate_t(N, center, std=1, df=1, rng=None):\n    \"\"\"Sample N point from a multivariate guassian with mean at center and\n    Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred\n    from user passed center.\n\n    Args:\n        N (int): number of points to sample\n        center (np.ndarray): d-dimensional point\n        std (float, optional): Standard deviation along any axis. Covariance is `std`*Identity Matrix. Defaults to 1.\n                                Also accepts numpy covariance matrix\n        df (float, optional): Degrees of freedom of the distribution. Defaults to 1. If np.inf results are multivariate normal.\n        rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.\n\n    Returns:\n        np.ndarray: N samples from multi-dimensional guassian centered at `center`. (N x d) matrix\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if not isinstance(std, np.ndarray):\n        d = center.shape[0]\n        COV = std * np.eye(d)\n    else:\n        COV = std\n\n    # X = rng.multivariate_normal(center, COV, size=N)\n    # dist = stats.multivariate_t(center, shape=COV, df=df, seed=rng)\n    # X = dist.rvs(size=size)\n    X = stats.multivariate_t.rvs(loc=center, shape=COV, df=df, size=N, random_state=rng)\n\n    return X\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.distributions","title":"<code>distributions</code>","text":""},{"location":"reference/datasets/#simnetpy.datasets.distributions.cluster_centers","title":"<code>cluster_centers(n, d, lower=1, higher=2, rng=None, init=None)</code>","text":"<p>Sample n point in d-dimensional space. Points will be between (lower, ~2*higher) distance from all other points. Initial center will be (0,0, ..,0) unless otherwise specified. Sampling done sequentially with next center proposed, rejected if too close to all others and resampled until accepted. If sampling large number of points &amp; time taken is large increase size of higher.</p> <p>Note: 2higher is not actual distance upper bound. higher controls size of box around previous center that we sample a proposal point. Each center sampled from box with sides of size 2higher. Args:     n (int): number of points to generate     d (int): number of dimensions     lower (float, optional): Lower bound of distances to accept. All points will be                         at least lower away from each other. Defaults to 1.     higher (float, optional): Size of box around previous center to sample from. Defaults to 2.     rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.     init (np.ndarray, optional): Location of first sample. Defaults to None. Note points are shuffled     but if none at least one will be the origin (0,0,...,0).</p> <p>Returns:</p> <ul> <li> <code>list</code>        \u2013          <p>n randomly sampled points in d-dimensional space all at least lower away from each other.</p> </li> </ul> Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def cluster_centers(n, d, lower=1, higher=2, rng=None, init=None):\n    \"\"\"Sample n point in d-dimensional space. Points will be between\n    (lower, ~2*higher) distance from all other points. Initial center will be (0,0, ..,0)\n    unless otherwise specified. Sampling done sequentially with next center proposed,\n    rejected if too close to all others and resampled until accepted.\n    If sampling large number of points &amp; time taken is large increase size of higher.\n\n    Note: 2*higher is not actual distance upper bound. higher controls size of box around previous center\n    that we sample a proposal point. Each center sampled from box with sides of size 2*higher.\n    Args:\n        n (int): number of points to generate\n        d (int): number of dimensions\n        lower (float, optional): Lower bound of distances to accept. All points will be\n                            at least lower away from each other. Defaults to 1.\n        higher (float, optional): Size of box around previous center to sample from. Defaults to 2.\n        rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.\n        init (np.ndarray, optional): Location of first sample. Defaults to None. Note points are shuffled\n        but if none at least one will be the origin (0,0,...,0).\n\n    Returns:\n        list: n randomly sampled points in d-dimensional space all at least lower away from each other.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if init is None:\n        init = np.zeros(d)\n    centers = []\n    x = init + uniform_sampler(d, std=higher, rng=rng)\n    centers.append(x)\n\n    while len(centers) &lt; n:\n        # sample point in random direction\n        x = uniform_sampler(d, std=higher, rng=rng)\n\n        # find random center and move away in direction x\n        i = rng.integers(0, len(centers))\n        x = centers[i] + x\n\n        carray = np.array(centers)\n        dist = np.linalg.norm(carray - x, axis=1, ord=2)\n\n        if np.all(dist &gt; lower):\n            centers.append(x)\n    rng.shuffle(centers)  # shuffle so first cluster is not necessarily close to init\n    return centers\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.distributions.mixed_categorical_clusters","title":"<code>mixed_categorical_clusters(N, d, nclusters=3, sizes='equal', alpha=5, beta=1, nlevels=5, return_skew_factors=True, rng=None)</code>","text":"<p>alpha and beta control shape of skew factor distribution higher max(abs(a,b))&gt;1 -&gt; less flat more peaked distribution a skewed below 0.5 a&gt;b -&gt; skewed above 0.5 so 5,1 means on average skew passed to ordinal feature generator will be centered on a/a+b~0.83 and 1,5 means average skew will be ~0.16 lower average skew means noisier data and harder clustering problem Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def mixed_categorical_clusters(\n    N,\n    d,\n    nclusters=3,\n    sizes=\"equal\",\n    alpha=5,\n    beta=1,\n    nlevels=5,\n    return_skew_factors=True,\n    rng=None,\n):\n    \"\"\"alpha and beta control shape of skew factor distribution\n    higher max(abs(a,b))&gt;1 -&gt; less flat more peaked distribution\n    a&lt;b -&gt; skewed below 0.5\n    a&gt;b -&gt; skewed above 0.5\n    so 5,1 means on average skew passed to ordinal feature generator will be centered on a/a+b~0.83\n    and 1,5 means average skew will be ~0.16\n    lower average skew means noisier data and harder clustering problem\n\n    \"\"\"\n\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if isinstance(sizes, str):\n        assert sizes.lower() in [\n            \"equal\",\n            \"random\",\n            \"roughly_equal\",\n        ], \"if specifying method sizes must be one of [equal, random, roughly_equal]\"\n        sizes = split_data_into_clusters(N, nclusters, method=sizes)\n    elif sizes is None:\n        sizes = split_data_into_clusters(N, nclusters, method=\"equal\")\n\n    assert sizes.shape[0] == nclusters, \"nclusters and sizes must match\"\n    assert sizes.sum() == N, f\"sizes must add up to N {sizes.sum()} != {N}\"\n\n    # N = sizes.sum()\n\n    # generate skew distribution &amp; sample\n    rv = stats.beta(a=alpha, b=beta)\n    skew_factors = rv.rvs(size=d, random_state=rng)\n\n    # generate features\n    X = np.zeros((N, d))\n    for i in range(d):\n        si = skew_factors[i]\n        if si &gt;= 0.5 and si &lt;= 1:\n            skew = (\n                2 * ((nlevels - 1) / nlevels) * si + (2 - nlevels) / nlevels\n            )  # map skew factor from [0.5, 1] to [1/nlevels, 1] i.e. si=0.5, skew=1/nlevels\n            X[:, i] = mixed_categorical_cluster_feature(\n                sizes, nlevels=nlevels, min_largest_cat=skew, rng=rng\n            )\n        elif si &gt;= 0 and si &lt; 0.5:\n            skew = (\n                2 * ((1 - nlevels) / nlevels) * si + 1\n            )  # map skew factor from [0.5, 0] to [1/nlevels, 1] i.e si=0 -&gt; skew = 1.0\n            X[:, i] = single_categorical_feature(\n                N, nlevels=nlevels, min_largest_cat=skew, rng=rng\n            )\n        else:\n            raise ValueError(\"Error beta distribution ill defined\")\n\n    # generate labels\n    y = np.zeros(N)\n    total = 0\n    for i, size in enumerate(sizes):\n        y[total : total + size] = i\n        total += size\n\n    dataset = Bunch(y=y, X=X)\n    if return_skew_factors:\n        dataset[\"skew_factors\"] = skew_factors\n\n    return dataset\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.distributions.multivariate_guassian","title":"<code>multivariate_guassian(N, center, std=1, rng=None)</code>","text":"<p>Sample N point from a multivariate guassian with mean at center and Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred from user passed center.</p> <p>Parameters:</p> <ul> <li> <code>N</code>             (<code>int</code>)         \u2013          <p>number of points to sample</p> </li> <li> <code>center</code>             (<code>ndarray</code>)         \u2013          <p>d-dimensional point</p> </li> <li> <code>std</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Standard deviation along any axis. Covariance is <code>std</code>*Identity Matrix. Defaults to 1.</p> </li> <li> <code>rng</code>             (<code>default_rng</code>, default:                 <code>None</code> )         \u2013          <p>user seeded random number generator. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: N samples from multi-dimensional guassian centered at <code>center</code>. (N x d) matrix</p> </li> </ul> Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def multivariate_guassian(N, center, std=1, rng=None):\n    \"\"\"Sample N point from a multivariate guassian with mean at center and\n    Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred\n    from user passed center.\n\n    Args:\n        N (int): number of points to sample\n        center (np.ndarray): d-dimensional point\n        std (float, optional): Standard deviation along any axis. Covariance is `std`*Identity Matrix. Defaults to 1.\n        rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.\n\n    Returns:\n        np.ndarray: N samples from multi-dimensional guassian centered at `center`. (N x d) matrix\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if not isinstance(std, np.ndarray):\n        d = center.shape[0]\n        COV = std * np.eye(d)\n    else:\n        COV = std\n\n    X = rng.multivariate_normal(center, COV, size=N)\n    return X\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.distributions.multivariate_t","title":"<code>multivariate_t(N, center, std=1, df=1, rng=None)</code>","text":"<p>Sample N point from a multivariate guassian with mean at center and Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred from user passed center.</p> <p>Parameters:</p> <ul> <li> <code>N</code>             (<code>int</code>)         \u2013          <p>number of points to sample</p> </li> <li> <code>center</code>             (<code>ndarray</code>)         \u2013          <p>d-dimensional point</p> </li> <li> <code>std</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Standard deviation along any axis. Covariance is <code>std</code>*Identity Matrix. Defaults to 1.                     Also accepts numpy covariance matrix</p> </li> <li> <code>df</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Degrees of freedom of the distribution. Defaults to 1. If np.inf results are multivariate normal.</p> </li> <li> <code>rng</code>             (<code>default_rng</code>, default:                 <code>None</code> )         \u2013          <p>user seeded random number generator. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: N samples from multi-dimensional guassian centered at <code>center</code>. (N x d) matrix</p> </li> </ul> Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def multivariate_t(N, center, std=1, df=1, rng=None):\n    \"\"\"Sample N point from a multivariate guassian with mean at center and\n    Covariance of std*Identity (i.e. circular guassian). Dimensions of guassian inferred\n    from user passed center.\n\n    Args:\n        N (int): number of points to sample\n        center (np.ndarray): d-dimensional point\n        std (float, optional): Standard deviation along any axis. Covariance is `std`*Identity Matrix. Defaults to 1.\n                                Also accepts numpy covariance matrix\n        df (float, optional): Degrees of freedom of the distribution. Defaults to 1. If np.inf results are multivariate normal.\n        rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.\n\n    Returns:\n        np.ndarray: N samples from multi-dimensional guassian centered at `center`. (N x d) matrix\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    if not isinstance(std, np.ndarray):\n        d = center.shape[0]\n        COV = std * np.eye(d)\n    else:\n        COV = std\n\n    # X = rng.multivariate_normal(center, COV, size=N)\n    # dist = stats.multivariate_t(center, shape=COV, df=df, seed=rng)\n    # X = dist.rvs(size=size)\n    X = stats.multivariate_t.rvs(loc=center, shape=COV, df=df, size=N, random_state=rng)\n\n    return X\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.distributions.uniform_sampler","title":"<code>uniform_sampler(d, std=1, rng=None)</code>","text":"<p>sample random location in d dimensional box with max value of std and min value -std on any axis</p> <p>Parameters:</p> <ul> <li> <code>d</code>             (<code>int</code>)         \u2013          <p>number of dimensions</p> </li> <li> <code>std</code>             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>max abs value on any axis. Defaults to 1.</p> </li> <li> <code>rng</code>             (<code>default_rng</code>, default:                 <code>None</code> )         \u2013          <p>user seeded random number generator. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: random point in d dimensional space with max abs value of std on any dimension</p> </li> </ul> Source code in <code>src/simnetpy/datasets/distributions.py</code> <pre><code>def uniform_sampler(d, std=1, rng=None):\n    \"\"\"sample random location in d dimensional box with\n    max value of std and min value -std on any axis\n\n    Args:\n        d (int): number of dimensions\n        std (float, optional): max abs value on any axis. Defaults to 1.\n        rng (np.random.default_rng, optional): user seeded random number generator. Defaults to None.\n\n    Returns:\n        np.ndarray: random point in d dimensional space with max abs value of std on any dimension\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    a = [1, -1]\n\n    direction = rng.choice(a, size=d)\n    r = rng.uniform(0, std, size=d)\n    return direction * r\n</code></pre>"},{"location":"reference/datasets/#simnetpy.datasets.multi_mod","title":"<code>multi_mod</code>","text":""},{"location":"reference/graph/","title":"Graph","text":""},{"location":"reference/graph/#graph","title":"Graph","text":""},{"location":"reference/graph/#simnetpy.graph.igraphf","title":"<code>igraphf</code>","text":""},{"location":"reference/graph/#simnetpy.graph.igraphf.Igraph","title":"<code>Igraph</code>","text":"<p>             Bases: <code>Graph</code></p> Source code in <code>src/simnetpy/graph/igraphf.py</code> <pre><code>class Igraph(ig.Graph):\n    def __init__(self, *args, **kwds):\n        X = kwds.pop(\"X\", None)  # check if X passed as argument\n\n        if isinstance(X, pd.DataFrame):\n            X = X.to_dict(orient=\"list\")\n\n        super().__init__(*args, **kwds)\n\n    def graph_stats(self):\n        ddict = {}\n        ddict[\"density\"] = self.density()\n        ddict[\"n\"] = self.vcount()\n        ddict[\"E\"] = self.ecount()\n        C = self.component_sizes(pcent=False)\n        ddict[\"Nc\"] = C.shape[1]\n        ddict[\"ncmax\"] = C[1, 0]\n        ddict[\"diameter\"] = self.diameter()\n        ddict[\"globalcc\"] = self.transitivity_undirected(mode=\"zero\")\n        ddict[\"avglocalcc\"] = self.transitivity_avglocal_undirected(mode=\"zero\")\n        ddict[\"assortativity\"] = self.assortativity_degree(directed=False)\n        ddict[\"avg_path_length\"] = self.average_path_length(directed=False)\n        dd = np.array(self.degree())\n        ddict[\"avg_degree\"] = dd.mean()\n        ddict[\"median_degree\"] = np.median(dd)\n        ddict[\"max_degree\"] = dd.max()\n\n        return ddict\n\n    def component_stats(self):\n        ddict = {}\n        ddict[\"E\"] = self.ecount()\n\n        cc = self.connected_components()\n        ddict[\"Nc\"] = len(cc)\n\n        idxmax, idx2 = self.find_first_second_largest_cc(cc.membership)\n\n        gc = cc.subgraph(idx=idxmax)\n        gc = self.from_igraph(gc)\n        ddict[\"cc_max\"] = gc.graph_stats()\n\n        ccmem = np.array(cc.membership)\n        ddict[\"cc_max_mem\"] = ccmem == idxmax\n\n        if idx2 is not None:\n            gc2 = cc.subgraph(idx=idx2)\n            gc2 = self.from_igraph(gc2)\n            ddict[\"cc2\"] = gc2.graph_stats()\n        else:\n            ddict[\"cc2\"] = None\n        return ddict\n\n    def component_sizes(self, cc=None, pcent=True):\n        if cc is None:\n            cc = self.connected_components()\n\n        C = np.vstack(np.unique(cc.membership, return_counts=True))\n        C = C[:, np.argsort(C[1, :])[::-1]]  # sort so largest is first\n        if pcent:\n            C = C.astype(\"float64\")\n            C[1, :] = C[1, :] / len(cc.membership)\n\n        return C\n\n    def large_components(self, large_comp_cutoff=0.1, cc=None, return_idx=False):\n        \"\"\"Find components in network greater than large_comp_cutoff.\n\n        Args:\n            large_comp_cutoff (float/int, optional): Cutoff to be considered a large component.\n                        If int is number of nodes, if float is percentage of total nodes.\n                        note decimal percentage so 33% cutoff would be 0.33.Defaults to 0.1\n            cc (ig.VertexClustering, optional): precalculated components/clustering.\n                        Typically output of g.connected_components. Defaults to None.\n            return_idx (bool, optional): flag wether to return indexs of components/clusters. Defaults to False.\n\n        Returns:\n            list/tuple of lists: if return idx returns tuple containing list of large component subgraphs and list of indexes\n                                otherwise returns just list of component subgraphs\n        \"\"\"\n        if cc is None:\n            cc = self.connected_components()\n\n        if isinstance(large_comp_cutoff, float):\n            pcent = True\n        elif isinstance(large_comp_cutoff, int):\n            pcent = False\n        else:\n            raise ValueError(\n                \"large_comp_cutoff should be node number or decimal percentage (i.e. 33% cutoff should be 0.33)\"\n            )\n\n        C = self.component_sizes(cc, pcent=pcent)\n\n        large_comps = C[0, C[1, :] &gt;= large_comp_cutoff]\n        components = [cc.subgraph(idx) for idx in large_comps]\n\n        return (components, large_comps) if return_idx else components\n\n    def find_first_second_largest_cc(self, cc=None):\n        if cc is None:\n            cc = self.connected_components()\n            cc = cc.membership\n\n        C = np.vstack(np.unique(cc, return_counts=True))\n        cmax = C[:, C[1, :].argmax()]\n        cc2 = C[\n            :, (C[1, :] &gt; 1) &amp; (C[1, :] &lt; cmax[1])\n        ]  # find values lower than max but greater than 1 (might be empty?)\n        try:\n            cc2 = cc2[:, cc2[1, :].argmax()]\n        except ValueError:\n            return cmax[0], None  # if no second component return None.\n\n        return cmax[0], cc2[0]  # returns index for largest and 2nd largest components\n\n    def get_comp(self, idx, cc=None):\n        if cc is None:\n            cc = self.connected_components()\n\n        gc = cc.subgraph(idx=idx)\n\n        return self.from_igraph(gc)\n\n    def max_component(self):\n        cc = self.connected_components()\n        idxmax, idx2 = self.find_first_second_largest_cc(cc.membership)\n        return self.get_comp(idx=idxmax, cc=cc)\n\n    def get_2ndmax_comp(self):\n        cc = self.connected_components()\n        idxmax, idx2 = self.find_first_second_largest_cc(cc.membership)\n        if idx2 is None:\n            raise ValueError(\"No 2nd largest component\")\n        return self.get_comp(idx=idx2, cc=cc)\n\n    def get_X(self):\n        X = []\n        for k in self.vs.attributes():\n            v = self.vs[k]\n            X.append(v)\n        X = np.array(X).T  # Transpose to have shape n x d\n        return X\n\n    def plot_degree_dist(self, logbinsize=0.1, LOG_ONLY=False):\n        degree = self.degree()\n        plot_degree_dist(degree_dist=degree, logbinsize=logbinsize, LOG_ONLY=LOG_ONLY)\n\n    def plottable(self, mode=\"undirected\"):\n        A = self.get_adjacency_sparse()\n        return ig.Graph().Adjacency(A, mode=mode)\n\n    @classmethod\n    def from_igraph(cls, gg, *args, **kwds):\n        return cls(*args, **kwds) + gg\n</code></pre>"},{"location":"reference/graph/#simnetpy.graph.igraphf.Igraph.large_components","title":"<code>large_components(large_comp_cutoff=0.1, cc=None, return_idx=False)</code>","text":"<p>Find components in network greater than large_comp_cutoff.</p> <p>Parameters:</p> <ul> <li> <code>large_comp_cutoff</code>             (<code>float / int</code>, default:                 <code>0.1</code> )         \u2013          <p>Cutoff to be considered a large component.         If int is number of nodes, if float is percentage of total nodes.         note decimal percentage so 33% cutoff would be 0.33.Defaults to 0.1</p> </li> <li> <code>cc</code>             (<code>VertexClustering</code>, default:                 <code>None</code> )         \u2013          <p>precalculated components/clustering.         Typically output of g.connected_components. Defaults to None.</p> </li> <li> <code>return_idx</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>flag wether to return indexs of components/clusters. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>list/tuple of lists: if return idx returns tuple containing list of large component subgraphs and list of indexes                 otherwise returns just list of component subgraphs</p> </li> </ul> Source code in <code>src/simnetpy/graph/igraphf.py</code> <pre><code>def large_components(self, large_comp_cutoff=0.1, cc=None, return_idx=False):\n    \"\"\"Find components in network greater than large_comp_cutoff.\n\n    Args:\n        large_comp_cutoff (float/int, optional): Cutoff to be considered a large component.\n                    If int is number of nodes, if float is percentage of total nodes.\n                    note decimal percentage so 33% cutoff would be 0.33.Defaults to 0.1\n        cc (ig.VertexClustering, optional): precalculated components/clustering.\n                    Typically output of g.connected_components. Defaults to None.\n        return_idx (bool, optional): flag wether to return indexs of components/clusters. Defaults to False.\n\n    Returns:\n        list/tuple of lists: if return idx returns tuple containing list of large component subgraphs and list of indexes\n                            otherwise returns just list of component subgraphs\n    \"\"\"\n    if cc is None:\n        cc = self.connected_components()\n\n    if isinstance(large_comp_cutoff, float):\n        pcent = True\n    elif isinstance(large_comp_cutoff, int):\n        pcent = False\n    else:\n        raise ValueError(\n            \"large_comp_cutoff should be node number or decimal percentage (i.e. 33% cutoff should be 0.33)\"\n        )\n\n    C = self.component_sizes(cc, pcent=pcent)\n\n    large_comps = C[0, C[1, :] &gt;= large_comp_cutoff]\n    components = [cc.subgraph(idx) for idx in large_comps]\n\n    return (components, large_comps) if return_idx else components\n</code></pre>"},{"location":"reference/plotting/","title":"Plotting","text":""},{"location":"reference/plotting/#plotting","title":"Plotting","text":""},{"location":"reference/plotting/#simnetpy.plotting.graph_plot","title":"<code>graph_plot</code>","text":""},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.color_nodes","title":"<code>color_nodes(ax, g, y, alpha=0.5)</code>","text":"<p>Color node patches based on cluster labels (uses current mpl style colors) applies alpha to nodes and edges</p>"},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.color_nodes--passing-color-to-igraph-does-not-work-with-matplotlib-backend","title":"passing color to igraph does not work with matplotlib backend","text":"<p>Parameters:</p> <ul> <li> <code>ax</code>             (<code>axis</code>)         \u2013          <p>axis to color vertex patches on</p> </li> <li> <code>g</code>             (<code>_type_</code>)         \u2013          <p>Igraph graph instance</p> </li> <li> <code>y</code>             (<code>list or ndarray</code>)         \u2013          <p>cluster labels</p> </li> <li> <code>alpha</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>alpha setting for nodes and edges. Defaults to 0.5.</p> </li> </ul> Source code in <code>src/simnetpy/plotting/graph_plot.py</code> <pre><code>def color_nodes(ax, g, y, alpha=0.5):\n    \"\"\"Color node patches based on cluster labels\n    (uses current mpl style colors)\n    applies alpha to nodes and edges\n    # passing color to igraph does not work with matplotlib backend\n\n    Args:\n        ax (plt.axis): axis to color vertex patches on\n        g (_type_): Igraph graph instance\n        y (list or np.ndarray): cluster labels\n        alpha (float, optional): alpha setting for nodes and edges. Defaults to 0.5.\n    \"\"\"\n    m = g.vcount() + g.ecount() # igraph plots first vertices then edges\n    cc = ax.get_children()\n\n    for i, c in enumerate(cc[:m]):\n        c.set_alpha(alpha)\n        # if a circle i.e. a vertex set color according to cluster membership\n        if isinstance(c, ptc.Circle):\n            c.set_edgecolor(f'C{y[i]}')\n            c.set_facecolor(f'C{y[i]}')\n</code></pre>"},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.find_cutoff","title":"<code>find_cutoff(step)</code>","text":"<p>Find lowest integer where log(k+1) - log(k) &lt; step</p> <p>Parameters:</p> <ul> <li> <code>step</code>             (<code>float</code>)         \u2013          <p>step taken in logspace</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>        \u2013          <p>cutoff below which integers increment more than step in logspace</p> </li> <li>         \u2013          <p>e.g. 5 cutoff for 0.1 as log(5) - log(4) = 0.097 but log(4) - log(3) &gt; 0.1</p> </li> </ul> Source code in <code>src/simnetpy/plotting/graph_plot.py</code> <pre><code>def find_cutoff(step):\n    \"\"\"Find lowest integer where log(k+1) - log(k) &lt; step\n\n    Args:\n        step (float): step taken in logspace\n\n    Returns:\n        int: cutoff below which integers increment more than *step* in logspace\n        e.g. 5 cutoff for 0.1 as log(5) - log(4) = 0.097 but log(4) - log(3) &gt; 0.1\n    \"\"\"\n    seq = np.arange(1,50)\n    d = np.diff(np.log10(seq))\n\n    cutoff = seq[1:][d&lt;step].min()\n    return cutoff\n</code></pre>"},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.get_log_bins","title":"<code>get_log_bins(x, y, step=0.1, verbose=False)</code>","text":"<p>Function to create log bins for a degree sequence</p> <p>Parameters:</p> <ul> <li> <code>x</code>             (<code>array</code>)         \u2013          <p>sequence of node degrees </p> </li> <li> <code>y</code>             (<code>array</code>)         \u2013          <p>corresponding counts of each degree</p> </li> <li> <code>step</code>             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>stepsize in logspace. Lower increase resolution. Min step supported 0.01 Defaults to 0.1.</p> </li> <li> <code>verbose</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Flag to print resolution and cutoff. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_type_</code>        \u2013          <p>description</p> </li> </ul> Source code in <code>src/simnetpy/plotting/graph_plot.py</code> <pre><code>def get_log_bins(x, y, step=0.1, verbose=False):\n    \"\"\"Function to create log bins for a degree sequence\n\n    Args:\n        x (np.array): sequence of node degrees \n        y (np.array): corresponding counts of each degree\n        step (float, optional): stepsize in logspace. Lower increase resolution. Min step supported 0.01 Defaults to 0.1.\n        verbose (bool, optional): Flag to print resolution and cutoff. Defaults to False.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    max_x = np.log10(x.max())\n    min_x = np.log10(x[x&gt;0].min())\n\n    # Find next point to the right of max_x if incrementing in steps\n    #  (e.g. 2.8 for max 2.73 when incrementing by 0.1)\n    endpoint = round(math.ceil(max_x/step)*step,3)\n    # find number of points for logspace to increase in steps of 0.1\n    numpoints = math.ceil((endpoint-min_x)/step)\n\n    # get bins\n    bins = np.logspace(min_x, endpoint, num=numpoints)\n    # find cutoff where an increment of 1 in linspace is less than step in logspace\n    cutoff = find_cutoff(step)\n\n    if verbose:\n        print(f\"Resolution: {step:.3f} Cutoff: {cutoff}\")\n\n    # only keep bins above cutoff\n    bins = bins[bins&gt;cutoff]\n\n    # get mean value for each bin where\n    # mean is sum of degree values / number of integers between the bin end points\n    xm, ym = log_bin_means(x, y, bins)\n    # combine degree &amp; count values with bin midpoints and average count\n    xlog = np.concatenate((x[x&lt;=cutoff], xm))\n    ylog = np.concatenate((y[x&lt;=cutoff], ym))\n    return xlog, ylog\n</code></pre>"},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.get_log_ticks","title":"<code>get_log_ticks(max_val, start=0)</code>","text":"<p>helper function to display log ticks on degree plot</p> Source code in <code>src/simnetpy/plotting/graph_plot.py</code> <pre><code>def get_log_ticks(max_val, start=0):\n    \"\"\"helper function to display log ticks on degree plot\"\"\"\n\n    if max_val &lt; 1:\n        NEG=True\n        exp = math.floor(np.log10(max_val))\n        logvals = np.linspace(start=exp, stop=0, num=np.abs(exp)+1)\n\n    else:\n        NEG=False\n        # tmp = max_val\n        # max_val = start\n        # start\n        exp = math.floor(np.log10(max_val))\n        logvals = np.linspace(start=start, stop=exp, num=exp+1)\n\n    # midvals are the points we place a tick. \n    # 1,2 correspond to 1,2 or 10, 20 or 100, 200 i.e. 1*10**x, 2*10**x, ...\n    midvals = np.log10([1,2,3,4,5,6,7,8,9])\n    ticks = np.tile(logvals,(midvals.shape[0],1))\n    for i in range(midvals.shape[0]):\n        if NEG:\n            ticks[i,:] = ticks[i,:] - midvals[i]\n        else:\n            ticks[i,:] = ticks[i,:] + midvals[i]\n    ticks = ticks.T.flatten()\n    return ticks\n</code></pre>"},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.log_bin_means","title":"<code>log_bin_means(x, y, bins)</code>","text":"<p>Function to calculate the mean y value in each bin.  Finds y values for which corresponding x values are within the bin (endpoint not included). Mean is taken over all integers falling within the bin.  Mean value and bin midpoints are returned. e.g. x: [2,4, 8], y: [2, 1, 1], bin = [2, 6, 12] gives [4, 9], [(2+1)/4, 1/6]</p> <p>Parameters:</p> <ul> <li> <code>x</code>             (<code>array</code>)         \u2013          <p>sequence of node degrees </p> </li> <li> <code>y</code>             (<code>array</code>)         \u2013          <p>corresponding counts of each degree</p> </li> <li> <code>bins</code>             (<code>array</code>)         \u2013          <p>sequence of bins to get average count in.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>(array, array)</code>         \u2013          <p>Returns (bin midpoints, mean value in bin)</p> </li> </ul> Source code in <code>src/simnetpy/plotting/graph_plot.py</code> <pre><code>def log_bin_means(x, y, bins):\n    \"\"\"Function to calculate the mean y value in each bin. \n    Finds y values for which corresponding x values are within the bin (endpoint not included).\n    Mean is taken over all integers falling within the bin. \n    Mean value and bin midpoints are returned.\n    e.g. x: [2,4, 8], y: [2, 1, 1], bin = [2, 6, 12]\n    gives [4, 9], [(2+1)/4, 1/6]\n\n    Args:\n        x (np.array): sequence of node degrees \n        y (np.array): corresponding counts of each degree\n        bins (np.array): sequence of bins to get average count in.\n\n    Returns:\n        (np.array, np.array): Returns (bin midpoints, mean value in bin)\n    \"\"\"\n    yhat = []\n    xmid = []\n    for start, end in zip(bins[:-1], bins[1:]):\n        midpoint = start + (end - start)/2\n        start = round(start)\n        end = round(end)\n        counts = y[(x&gt;=start) &amp; (x &lt; end)]\n        n = end - start\n        mean = counts.sum()/n\n        yhat.append(mean)\n        xmid.append(midpoint)\n\n    return np.array(xmid), np.array(yhat)\n</code></pre>"},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.plot_degree_dist","title":"<code>plot_degree_dist(degree_dist, logbinsize=0.1, LOG_ONLY=False)</code>","text":"<p>Plot degree distribution as histogram and log-log scatter plot. Linear and log bin sequence shown for log-log plot</p> <p>Parameters:</p> <ul> <li> <code>degree_dist</code>             (<code>numpy array</code>)         \u2013          <p>sequence of node degrees.</p> </li> <li> <code>logbinsize</code>             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>stepsize in logspace. Lower increase resolution.                      Min step supported 0.01 Defaults to 0.1.</p> </li> </ul> Source code in <code>src/simnetpy/plotting/graph_plot.py</code> <pre><code>def plot_degree_dist(degree_dist, logbinsize=0.1, LOG_ONLY=False):\n    \"\"\"\n    Plot degree distribution as histogram and log-log scatter plot.\n    Linear and log bin sequence shown for log-log plot\n\n    Args:\n        degree_dist (numpy array): sequence of node degrees.\n        logbinsize (float, optional): stepsize in logspace. Lower increase resolution. \n                                Min step supported 0.01 Defaults to 0.1.\n    \"\"\"\n    deg, cnts = np.unique(degree_dist, return_counts=True)\n\n    if LOG_ONLY:\n        fig, ax = plt.subplots(1,1, figsize=(8,6))\n\n        sns.scatterplot(x=np.log10(deg), y=np.log10(cnts), ax=ax, alpha=0.7)\n        ax.set_ylabel('Count', labelpad=15, fontsize=15)\n        ax.set_xlabel('Degree', labelpad=15, fontsize=15)\n\n        ## log binned dist\n        deg, cnts = deg[1:], cnts[1:] # skip degree 0 as log(0) = inf\n\n        x, y =  get_log_bins(deg, cnts, step=logbinsize)\n        sns.scatterplot(x=np.log10(x), y=np.log10(y), ax=ax, alpha=0.7)\n        # ax.plot(np.log10(x), np.log10(y), 'ro')\n\n        # get log ticks to put on axis\n        xticks = get_log_ticks(deg.max())\n        yticks = get_log_ticks(cnts.max())\n        ax.set_xticks(xticks)\n        ax.set_yticks(yticks)\n        # display ticks as 10 to the power rather than 10, 100 etc\n        formatter = lambda x, pos: f'{10 ** x:g}' if not x % 1 else ''\n        ax.get_xaxis().set_major_formatter(formatter)\n        ax.get_yaxis().set_major_formatter(formatter)\n\n        # lims = np.log10(np.array([0.005, 25.]))\n        # ax.set_xlim(lims)\n        # ax.set_ylim(lims)\n        # include degree distribution statistics\n        # fig.text(0.8, 0.29, degstring, size=12)\n        fig.suptitle('Degree Distribution', size=20)\n        fig.show()\n    else:\n        # Plot degree distribution as lin-lin and log-log\n        fig, ax = plt.subplots(2,figsize=(10, 8))\n\n        # Fig 1 Linear distribution\n        i=0\n        sns.histplot(x=degree_dist, kde=True, binwidth=2, stat='probability',ax=ax[i])\n        ax[i].set_ylabel('Probability', labelpad=15, fontsize=15)\n        # include graph information\n        # fig.text(0.8, 0.73, graphstring, size=12)\n\n        # Fig 2 Log-Log distribution\n        i = 1\n        deg, cnts = deg[1:], cnts[1:] # skip degree 0 as log(0) = inf\n        sns.scatterplot(x=np.log10(deg), y=np.log10(cnts), ax=ax[i], alpha=0.7)\n        ax[i].set_ylabel('Count', labelpad=15, fontsize=15)\n        ax[i].set_xlabel('Degree', labelpad=15, fontsize=15)\n\n        ## log binned dist\n        x, y =  get_log_bins(deg, cnts, step=logbinsize)\n        sns.scatterplot(x=np.log10(x), y=np.log10(y), ax=ax[i], alpha=0.7)\n        # ax[i].plot(np.log10(x), np.log10(y), 'ro')\n\n        # get log ticks to put on axis\n        xticks = get_log_ticks(deg.max())\n        yticks = get_log_ticks(cnts.max())\n        ax[i].set_xticks(xticks)\n        ax[i].set_yticks(yticks)\n        # display ticks as 10 to the power rather than 10, 100 etc\n        formatter = lambda x, pos: f'{10 ** x:g}' if not x % 1 else ''\n        ax[i].get_xaxis().set_major_formatter(formatter)\n        ax[i].get_yaxis().set_major_formatter(formatter)\n\n        # lims = np.log10(np.array([0.005, 25.]))\n        # ax.set_xlim(lims)\n        # ax.set_ylim(lims)\n        # include degree distribution statistics\n        # fig.text(0.8, 0.29, degstring, size=12)\n        fig.suptitle('Degree Distribution', size=20)\n        fig.show()\n</code></pre>"},{"location":"reference/plotting/#simnetpy.plotting.graph_plot.plot_graph_colored_nodes","title":"<code>plot_graph_colored_nodes(g, y=None, ax=None, vs=None, alpha=0.5)</code>","text":"<p>Plot network and cluster nodes</p> <p>Parameters:</p> <ul> <li> <code>g</code>             (<code>_type_</code>)         \u2013          <p>network to plot</p> </li> <li> <code>y</code>             (<code>_type_</code>, default:                 <code>None</code> )         \u2013          <p>cluster labels. Defaults to one single cluster.</p> </li> <li> <code>ax</code>             (<code>_type_</code>, default:                 <code>None</code> )         \u2013          <p>axis to plot on if given. Defaults to None.</p> </li> <li> <code>vs</code>             (<code>_type_</code>, default:                 <code>None</code> )         \u2013          <p>visual style dict for igraph. Defaults to DEFAULT_VS if nothing passed.</p> </li> <li> <code>alpha</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>level of transparancey for nodes and edges. Defaults to 0.5.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>fig, ax: if ax passed fig is None.</p> </li> </ul> Source code in <code>src/simnetpy/plotting/graph_plot.py</code> <pre><code>def plot_graph_colored_nodes(g, y=None, ax=None, vs=None, alpha=0.5):\n    \"\"\"Plot network and cluster nodes\n\n    Args:\n        g (_type_): network to plot\n        y (_type_, optional): cluster labels. Defaults to one single cluster.\n        ax (_type_, optional): axis to plot on if given. Defaults to None.\n        vs (_type_, optional): visual style dict for igraph. Defaults to DEFAULT_VS if nothing passed.\n        alpha (float, optional): level of transparancey for nodes and edges. Defaults to 0.5.\n\n    Returns:\n        fig, ax: if ax passed fig is None.\n    \"\"\"\n    if vs is None:\n        vs = DEFAULT_VS\n\n    if y is None:\n        y = np.zeros(g.vcount(), dtype=np.int8)\n\n    if ax is None:\n        fig, ax = plt.subplots()\n    else:\n        fig = None\n\n    # if isinstance(g, Igraph):\n    #     g = g.plottable()\n\n    try:\n        g = g.plottable()\n    except AttributeError:\n        g = g\n\n    ig.plot(g, target=ax, **vs)\n    color_nodes(ax, g, y, alpha)\n\n    return fig, ax\n</code></pre>"},{"location":"reference/similarity/","title":"Similarity","text":""},{"location":"reference/similarity/#similarity","title":"Similarity","text":""},{"location":"reference/similarity/#simnetpy.similarity.merging","title":"<code>merging</code>","text":""},{"location":"reference/similarity/#simnetpy.similarity.similarity","title":"<code>similarity</code>","text":""},{"location":"reference/similarity/#simnetpy.similarity.similarity.chi_square","title":"<code>chi_square(pu, pv)</code>","text":"<p>Chi-square distance or histogram distance. d(u,v) = 1/2 \\sum_i=0^N (p(u_i) - p(v_i))**2/(p(u_i)+p(v_i))</p> <p>Parameters:</p> <ul> <li> <code>pu</code>             (<code>array</code>)         \u2013          <p>vector of probabilities of observing elements of u</p> </li> <li> <code>pv</code>             (<code>array</code>)         \u2013          <p>vector of probabilities of observing elements of u</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>        \u2013          <p>distance between u and v</p> </li> </ul> Source code in <code>src/simnetpy/similarity/similarity.py</code> <pre><code>def chi_square(pu, pv):\n    \"\"\"Chi-square distance or histogram distance.\n    d(u,v) = 1/2 \\sum_i=0^N (p(u_i) - p(v_i))**2/(p(u_i)+p(v_i))\n\n    Args:\n        pu (np.array): vector of probabilities of observing elements of u\n        pv (np.array): vector of probabilities of observing elements of u\n\n    Returns:\n        float: distance between u and v\n    \"\"\"\n    x = pu + pv\n    x[x==0] = 1 # if u and v can't be negative so if u+v=0 then u is the same as v and distance is 0. \n    # set totals to 1 in this case to allow computation.\n\n    d = ((pu - pv)**2/(x)).sum()\n    d /= 2\n    return d\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.similarity.multi_modal_similarity","title":"<code>multi_modal_similarity(data, N, method, idxmap=None, norm=True)</code>","text":"<p>Compute pairwise similarity for each data modality Uses same metric on each modality. </p> <p>Note all matrices in data must be NxN if idxmap not specified Args:     data (dict): dictionary of Nmodality feature matrices (N x d_i np.ndarrays)     N (int): number of individuals in pairwise calculation     idxmap (dict): dictionary contain index of each matrix in larger set</p> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: (Nmodality, N, N) pairwise similarity matrix</p> </li> </ul> Source code in <code>src/simnetpy/similarity/similarity.py</code> <pre><code>def multi_modal_similarity(data, N, method, idxmap=None, norm=True):\n    \"\"\"Compute pairwise similarity for each data modality\n    Uses same metric on each modality. \n\n    Note all matrices in data must be NxN if idxmap not specified\n    Args:\n        data (dict): dictionary of Nmodality feature matrices (N x d_i np.ndarrays)\n        N (int): number of individuals in pairwise calculation\n        idxmap (dict): dictionary contain index of each matrix in larger set\n\n    Returns:\n        np.ndarray: (Nmodality, N, N) pairwise similarity matrix\n    \"\"\"\n    Nm = len(data) # number modalities\n    S = np.empty((Nm,N,N), dtype=np.float32)\n    S.fill(np.nan)\n\n    for i,(key, X) in enumerate(data.items()):\n        D = pairwise_sim(X,method, norm=norm)\n        if idxmap is not None:\n            idx = idxmap[key]\n            j,k = np.meshgrid(idx,idx)\n            S[i, j, k] = D \n        else:\n            S[i,:,:] = D\n    return S\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.similarity.partial_mm_similarity","title":"<code>partial_mm_similarity(data, metric, norm=True, snf_aff=False, K=20, mu=0.5)</code>","text":"<p>Calculate multi-modal similarity where rows in certain modalities might be mssing.  Can use normal distance metric of Affinity proposed in SNF (Similarity Network Fusion).</p> <p>Note: Function returns a m x N x N dissimilarity matrix. If affinity then S = -(Affinity Matrix)</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>list</code>)         \u2013          <p>array of data matrices for each modality. Each array should be N x d.          if data missing include NaN rows in input.</p> </li> <li> <code>metric</code>             (<code>_type_</code>)         \u2013          <p>metric to use in distance calculation.</p> </li> <li> <code>norm</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>description. Defaults to True.</p> </li> <li> <code>snf_aff</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>description. Defaults to False.</p> </li> <li> <code>K</code>             (<code>int</code>, default:                 <code>20</code> )         \u2013          <p>description. Defaults to 20.</p> </li> <li> <code>mu</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>description. Defaults to 0.5.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_type_</code>        \u2013          <p>description</p> </li> </ul> Source code in <code>src/simnetpy/similarity/similarity.py</code> <pre><code>def partial_mm_similarity(data, metric, norm=True, snf_aff=False, K=20, mu=0.5):\n    \"\"\"Calculate multi-modal similarity where rows in certain modalities might be mssing. \n    Can use normal distance metric of Affinity proposed in SNF (Similarity Network Fusion).\n\n    Note: Function returns a m x N x N dissimilarity matrix. If affinity then S = -(Affinity Matrix)\n\n    Args:\n        data (list): array of data matrices for each modality. Each array should be N x d. \n                    if data missing include NaN rows in input.\n        metric (_type_): metric to use in distance calculation.\n        norm (bool, optional): _description_. Defaults to True.\n        snf_aff (bool, optional): _description_. Defaults to False.\n        K (int, optional): _description_. Defaults to 20.\n        mu (float, optional): _description_. Defaults to 0.5.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    Nm = len(data)\n    N, d = data[0].shape\n    S = np.empty((Nm, N, N))\n    S.fill(np.nan)\n    for i, X in enumerate(data):\n        idx = utils.non_nan_indices(X)\n        if snf_aff:\n            D = -snf_affinity(X[idx,:], metric=metric, K=K, mu=mu)    \n        else:\n            D = pairwise_sim(X[idx,:], metric, norm=norm)\n        j,k = np.meshgrid(idx,idx)\n        S[i, j, k] = D\n    return S\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold","title":"<code>threshold</code>","text":""},{"location":"reference/similarity/#simnetpy.similarity.threshold.combined_adj","title":"<code>combined_adj(D, K, t)</code>","text":"<p>Create a network from a dissimilarity matrix through a mixture of KNN and global threshold.</p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn dissimilarity matrix. smaller values =&gt; more similar.</p> </li> <li> <code>K</code>             (<code>int</code>)         \u2013          <p>Number of Neighbours to find for each individual</p> </li> <li> <code>t</code>             (<code>float</code>)         \u2013          <p>0 to 1 top 100*t% of edges to keep. 0.01 means top 1% most similar connections.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: Adjacency matrix of 0 and 1s</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def combined_adj(D, K, t):\n    \"\"\"Create a network from a dissimilarity matrix through a mixture of KNN and global\n    threshold.\n\n    Args:\n        D (np.ndarray): nxn dissimilarity matrix. smaller values =&gt; more similar.\n        K (int): Number of Neighbours to find for each individual\n        t (float): 0 to 1 top 100*t% of edges to keep. 0.01 means top 1% most similar connections.\n\n    Returns:\n        np.ndarray: Adjacency matrix of 0 and 1s\n    \"\"\"\n    A_k = knn_adj(D, K)\n\n    A_t = threshold_adj(D, t)\n    A = A_k + A_t\n    A[A&gt;1] = 1\n    return A\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.knn_adj","title":"<code>knn_adj(D, K)</code>","text":"<p>Create a network from a dissimilarity matrix by finding top K most similar neighbours for each node.  Note: uses brute force algorithm. Checks all possible values. Slow for large matrices</p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn dissimilarity matrix. smaller values =&gt; more similar.</p> </li> <li> <code>K</code>             (<code>int</code>)         \u2013          <p>Number of Neighbours to find for each individual</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: Adjacency matrix of 0 and 1s</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def knn_adj(D,K):\n    \"\"\"Create a network from a dissimilarity matrix by finding top K most similar neighbours for each node. \n    Note: uses brute force algorithm. Checks all possible values. Slow for large matrices\n\n    Args:\n        D (np.ndarray): nxn dissimilarity matrix. smaller values =&gt; more similar.\n        K (int): Number of Neighbours to find for each individual\n\n    Returns:\n        np.ndarray: Adjacency matrix of 0 and 1s\n    \"\"\"\n    assert isinstance(K, (int, np.integer)), \"K must be an integer\"\n\n    # Note D is distance matrix not affinity\n    # lower values =&gt; more similar.\n    # output from pairwise_sim has diagonals with minimum value \n    np.fill_diagonal(D, D.max()) # don't want to include diagonals\n\n    A = np.zeros(D.shape)\n    # find K nearest neighbours for each individual \n    nn = np.argsort(D, axis=1)[:,:K]\n    # add edge (set value = 1) for each nearest neighbour\n    np.put_along_axis(A,nn, 1, axis=1) \n    # make symmetric\n    A = A.T + A\n    # if two individuals both have each other as nn then value will be 2. set == 1\n    A[A&gt;1] = 1\n    return A\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.log_skewed_knn_adj","title":"<code>log_skewed_knn_adj(D, K, statNN=10, stat='mean', Kquantile=1.0)</code>","text":"<p>Create a network from a dissimilarity matrix through a mixture of KNN and global threshold.</p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn dissimilarity matrix. smaller values =&gt; more similar.</p> </li> <li> <code>K</code>             (<code>int</code>)         \u2013          <p>Control number of Neighbours in neighbour distribution. Coupled with Kquantile.  e.g. K=5, Kquantile=0.5 means 5 will be mean of distribution. kquantile=1.0 mean 5 will be max. </p> </li> <li> <code>statNN</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>Number of neighbours in stat calc. Defaults to 10.</p> </li> <li> <code>stat</code>             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>Stat to calculate from neighest neighbours, one of mean, median, std.                  Defaults to 'mean'.</p> </li> <li> <code>Kquantile</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>Quantile of stat dist to map K to. Defaults to 1.0.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: Adjacency matrix of 0 and 1s</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def log_skewed_knn_adj(D, K, statNN=10, stat='mean', Kquantile=1.0):\n    \"\"\"Create a network from a dissimilarity matrix through a mixture of KNN and global\n    threshold.\n\n    Args:\n        D (np.ndarray): nxn dissimilarity matrix. smaller values =&gt; more similar.\n        K (int): Control number of Neighbours in neighbour distribution. Coupled with Kquantile. \n            e.g. K=5, Kquantile=0.5 means 5 will be mean of distribution. kquantile=1.0 mean 5 will be max. \n        statNN (int, optional): Number of neighbours in stat calc. Defaults to 10.\n        stat (str, optional): Stat to calculate from neighest neighbours, one of mean, median, std. \n                            Defaults to 'mean'.\n        Kquantile (float, optional): Quantile of stat dist to map K to. Defaults to 1.0.\n\n    Returns:\n        np.ndarray: Adjacency matrix of 0 and 1s\n    \"\"\"\n    assert isinstance(K,int), \"K must be an integer\"\n\n    np.fill_diagonal(D,D.max())\n    NN = nn_distribution(D, K, statNN=statNN, stat=stat, Kquantile=Kquantile, mapping='log')\n    D_sorted = np.argsort(D, axis=1)\n    A = np.zeros(D.shape)\n    for i, nn in enumerate(NN):\n        idx = D_sorted[i,:nn]\n        A[i, idx] = 1\n\n    A = A.T + A\n    A[A&gt;1] = 1\n    return A\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.network_from_sim_mat","title":"<code>network_from_sim_mat(D, method='knn', **kwargs)</code>","text":"<p>function to sparsify dissimilarity matrix into adjacency </p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn Dissimilarity matrix. Smaller =&gt; more similar</p> </li> <li> <code>method</code>             (<code>str</code>, default:                 <code>'knn'</code> )         \u2013          <p>method to use to sparsify matrix.          one of [knn, threshold, combined, skewed_knn]. Defaults to 'knn'.</p> </li> <li> <code>**kwargs</code>         \u2013          <p>keyword arguments for sparsifying functions</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>ig.Graph: Graph created from similarity matrix</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def network_from_sim_mat(D, method='knn', **kwargs):\n    \"\"\"\n    function to sparsify dissimilarity matrix into adjacency \n\n    Args:\n        D (np.ndarray): nxn Dissimilarity matrix. Smaller =&gt; more similar\n        method (str, optional): method to use to sparsify matrix. \n                    one of [knn, threshold, combined, skewed_knn]. Defaults to 'knn'.\n        **kwargs: keyword arguments for sparsifying functions\n\n    Returns:\n        ig.Graph: Graph created from similarity matrix\n    \"\"\"\n    A = sparsify_sim_matrix(D, method=method, **kwargs)\n    g = mat2graph(A)\n    return g\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.nn_distribution","title":"<code>nn_distribution(D, K, statNN=10, stat='mean', Kquantile=1.0, mapping='linear')</code>","text":"<p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn dissimilarity matrix. smaller values =&gt; more similar.</p> </li> <li> <code>K</code>             (<code>int</code>)         \u2013          <p>Control number of Neighbours in neighbour distribution. Coupled with Kquantile.  e.g. K=5, Kquantile=0.5 means 5 will be mean of distribution. kquantile=1.0 mean 5 will be max. </p> </li> <li> <code>statNN</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>Number of neighbours in stat calc. Defaults to 10.</p> </li> <li> <code>stat</code>             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>Stat to calculate from neighest neighbours, one of mean, median, std.                  Defaults to 'mean'.</p> </li> <li> <code>Kquantile</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>Quantile of stat dist to map K to. Defaults to 1.0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_type_</code>        \u2013          <p>description</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def nn_distribution(D, K, statNN=10, stat='mean', Kquantile=1.0, mapping='linear'):\n    \"\"\"\n\n    Args:\n        D (np.ndarray): nxn dissimilarity matrix. smaller values =&gt; more similar.\n        K (int): Control number of Neighbours in neighbour distribution. Coupled with Kquantile. \n            e.g. K=5, Kquantile=0.5 means 5 will be mean of distribution. kquantile=1.0 mean 5 will be max. \n        statNN (int, optional): Number of neighbours in stat calc. Defaults to 10.\n        stat (str, optional): Stat to calculate from neighest neighbours, one of mean, median, std. \n                            Defaults to 'mean'.\n        Kquantile (float, optional): Quantile of stat dist to map K to. Defaults to 1.0.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    stat = stat.lower()\n    assert stat in ['mean', 'median', 'std'], 'Must be one of mean, median, std'\n    assert Kquantile &lt;= 1.0 and Kquantile&gt;=0, \"Must be between 0 and 1\"\n    assert mapping in ['linear', 'log'], \"mapping must be one of [linear, log]\"\n    S = neighbour_stat(D, k=statNN, stat=stat) # find average similarity amongst statNN closest neighbours\n    V = - (S - S.min())/(S.max()-S.min()) + 1 # normalise to 0, 1. Note: close to 1 means larger number of similar neighbours\n\n    # We map the distribution from to 0,Kmax so that the number of neighbours each node is assigned goes from 0, Kmax\n    Kq = np.quantile(V, Kquantile)\n    Kmax = int(K/Kq) \n    if mapping == 'linear':\n        V = Kmax*V # normalise to 0,K\n        NN = np.digitize(V, bins=np.arange(Kmax))\n    elif mapping == 'log':\n        upper = np.log(Kmax+1)\n        V = upper*V # normalise to 0,log(Kmax+1)\n        V = np.exp(V) # V now in [1, Kmax+1]\n        NN = np.digitize(V-1, bins=np.arange(Kmax)) # digitize maps x in [0-1] to 1 \n                                        #i.e. rounds up so need -1 to get correct range\n    return NN\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.skewed_knn_adj","title":"<code>skewed_knn_adj(D, K, statNN=10, stat='mean', Kquantile=1.0)</code>","text":"<p>Create a network from a dissimilarity matrix through a mixture of KNN and global threshold.</p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn dissimilarity matrix. smaller values =&gt; more similar.</p> </li> <li> <code>K</code>             (<code>int</code>)         \u2013          <p>Control number of Neighbours in neighbour distribution. Coupled with Kquantile.  e.g. K=5, Kquantile=0.5 means 5 will be mean of distribution. kquantile=1.0 mean 5 will be max. </p> </li> <li> <code>statNN</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>Number of neighbours in stat calc. Defaults to 10.</p> </li> <li> <code>stat</code>             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>Stat to calculate from neighest neighbours, one of mean, median, std.                  Defaults to 'mean'.</p> </li> <li> <code>Kquantile</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>Quantile of stat dist to map K to. Defaults to 1.0.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: Adjacency matrix of 0 and 1s</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def skewed_knn_adj(D, K, statNN=10, stat='mean', Kquantile=1.0):\n    \"\"\"Create a network from a dissimilarity matrix through a mixture of KNN and global\n    threshold.\n\n    Args:\n        D (np.ndarray): nxn dissimilarity matrix. smaller values =&gt; more similar.\n        K (int): Control number of Neighbours in neighbour distribution. Coupled with Kquantile. \n            e.g. K=5, Kquantile=0.5 means 5 will be mean of distribution. kquantile=1.0 mean 5 will be max. \n        statNN (int, optional): Number of neighbours in stat calc. Defaults to 10.\n        stat (str, optional): Stat to calculate from neighest neighbours, one of mean, median, std. \n                            Defaults to 'mean'.\n        Kquantile (float, optional): Quantile of stat dist to map K to. Defaults to 1.0.\n\n    Returns:\n        np.ndarray: Adjacency matrix of 0 and 1s\n    \"\"\"\n    assert isinstance(K,int), \"K must be an integer\"\n\n    np.fill_diagonal(D,D.max())\n    NN = nn_distribution(D, K, statNN=statNN, stat=stat, Kquantile=Kquantile, mapping='linear')\n    D_sorted = np.argsort(D, axis=1)\n    A = np.zeros(D.shape)\n    for i, nn in enumerate(NN):\n        idx = D_sorted[i,:nn]\n        A[i, idx] = 1\n\n    A = A.T + A\n    A[A&gt;1] = 1\n    return A\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.sparsify_sim_matrix","title":"<code>sparsify_sim_matrix(D, method='knn', **kwargs)</code>","text":"<p>function to sparsify dissimilarity matrix into adjacency </p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn Dissimilarity matrix. Smaller =&gt; more similar</p> </li> <li> <code>method</code>             (<code>str</code>, default:                 <code>'knn'</code> )         \u2013          <p>method to use to sparsify matrix.          one of [knn, threshold, combined, skewed_knn]. Defaults to 'knn'.</p> </li> <li> <code>**kwargs</code>         \u2013          <p>keyword arguments for sparsifying functions</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: nxn symmetric Adjacency matrix of 0s and 1s</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def sparsify_sim_matrix(D, method='knn', **kwargs):\n    \"\"\"\n    function to sparsify dissimilarity matrix into adjacency \n\n    Args:\n        D (np.ndarray): nxn Dissimilarity matrix. Smaller =&gt; more similar\n        method (str, optional): method to use to sparsify matrix. \n                    one of [knn, threshold, combined, skewed_knn]. Defaults to 'knn'.\n        **kwargs: keyword arguments for sparsifying functions\n\n    Returns:\n        np.ndarray: nxn symmetric Adjacency matrix of 0s and 1s\n    \"\"\"\n    fsparser = {'knn': knn_adj, 'threshold':threshold_adj, \n        'combined':combined_adj, 'skewed_knn':skewed_knn_adj, \n        'log_skewed_knn':log_skewed_knn_adj}\n    A = fsparser[method](D, **kwargs)\n    return A\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.threshold_adj","title":"<code>threshold_adj(D, t)</code>","text":"<p>Threshold dissimilarity matrix using quantile of values. Assumes distance. Edges retained are values below smallest t%.</p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn dissimilarity matrix. smaller values =&gt; more similar.</p> </li> <li> <code>t</code>             (<code>float</code>)         \u2013          <p>0 to 1 top 100*t% of edges to keep. 0.01 means top 1% most similar connections.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: Adjacency matrix of 0 and 1s</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def threshold_adj(D, t):\n    \"\"\"Threshold dissimilarity matrix using quantile of values. Assumes distance. Edges retained are values below\n    smallest t%.\n\n    Args:\n        D (np.ndarray): nxn dissimilarity matrix. smaller values =&gt; more similar.\n        t (float): 0 to 1 top 100*t% of edges to keep. 0.01 means top 1% most similar connections.\n\n    Returns:\n        np.ndarray: Adjacency matrix of 0 and 1s\n    \"\"\"\n    A = np.zeros(D.shape)\n    # d = np.triu(D).flatten()\n    np.fill_diagonal(D,0) # coversion to squareform requires 0 diagonals\n    d = squareform(D)\n    t = np.quantile(d, t)\n    A[D &lt; t] = 1\n    return A\n</code></pre>"},{"location":"reference/similarity/#simnetpy.similarity.threshold.threshold_graph","title":"<code>threshold_graph(D, t)</code>","text":"<p>Threshold dissimilarity matrix using quantile of values and create a igraph network. Assumes distance. Edges retained are values below smallest t%.</p> <p>Parameters:</p> <ul> <li> <code>D</code>             (<code>ndarray</code>)         \u2013          <p>nxn dissimilarity matrix. smaller values =&gt; more similar.</p> </li> <li> <code>t</code>             (<code>float</code>)         \u2013          <p>0 to 1 top 100*t% of edges to keep. 0.01 means top 1% most similar connections.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>ig.Graph: Graph created from thresholding connections.</p> </li> </ul> Source code in <code>src/simnetpy/similarity/threshold.py</code> <pre><code>def threshold_graph(D,t):\n    \"\"\"Threshold dissimilarity matrix using quantile of values and create a igraph network. Assumes distance. Edges retained are values below\n    smallest t%.\n\n    Args:\n        D (np.ndarray): nxn dissimilarity matrix. smaller values =&gt; more similar.\n        t (float): 0 to 1 top 100*t% of edges to keep. 0.01 means top 1% most similar connections.\n\n    Returns:\n        ig.Graph: Graph created from thresholding connections.\n    \"\"\"\n    A = threshold_adj(D,t)\n    g = mat2graph(A)\n    return g\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":""},{"location":"reference/utils/#utils","title":"Utils","text":""},{"location":"reference/utils/#simnetpy.utils.filesys","title":"<code>filesys</code>","text":""},{"location":"reference/utils/#simnetpy.utils.filesys.create_dirs_on_path","title":"<code>create_dirs_on_path(f, create_parent_if_file=True)</code>","text":"<p>function to create directories on given path if don't exist. Can be file or directory.  If file needs create_parent_if_file flag</p> <p>Parameters:</p> <ul> <li> <code>f</code>             (<code>Path or str</code>)         \u2013          <p>path to create dict on. can be dictionary or file</p> </li> <li> <code>create_parent_if_file</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if f is a file create parent directory. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotADirectoryError</code>           \u2013          <p>raises and error if f is file that has no suffix</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>pathlib.Path: path with all directories created</p> </li> </ul> Source code in <code>src/simnetpy/utils/filesys.py</code> <pre><code>def create_dirs_on_path(f, create_parent_if_file=True):\n    \"\"\"\n    function to create directories on given path if don't exist. Can be file or directory. \n    If file needs create_parent_if_file flag\n\n    Args:\n        f (pathlib.Path or str): path to create dict on. can be dictionary or file\n        create_parent_if_file (bool, optional): if f is a file create parent directory. Defaults to True.\n\n    Raises:\n        NotADirectoryError: raises and error if f is file that has no suffix\n\n    Returns:\n        pathlib.Path: path with all directories created\n    \"\"\"\n    p = pathlib.Path(f)\n    if p.suffix != '':\n        if create_parent_if_file:\n            p.parent.mkdir(parents=True, exist_ok=True)\n            return p\n        else:\n            raise NotADirectoryError\n\n    p.mkdir(parents=True, exist_ok=True)\n    return p\n</code></pre>"},{"location":"reference/utils/#simnetpy.utils.filesys.create_experiment_folder","title":"<code>create_experiment_folder(path, timestamp=True, config_path=None, copysrc=True, src=SRCDIR)</code>","text":"<p>Create experiment folder. Append timestamp to make name unique, copy src code to folder. copy config to folder</p> <p>Parameters:</p> <ul> <li> <code>path</code>             (<code>str or Path</code>)         \u2013          <p>name/path for experiment output</p> </li> <li> <code>src</code>             (<code>_type_</code>, default:                 <code>SRCDIR</code> )         \u2013          <p>location of source code that generated the model. Defaults to HOME/phd/phd-year2/src/phd_year2.</p> </li> <li> <code>config</code>             (<code>str or Path</code>)         \u2013          <p>location of config file.</p> </li> </ul> Source code in <code>src/simnetpy/utils/filesys.py</code> <pre><code>def create_experiment_folder(path, timestamp=True, config_path=None, copysrc=True, src=SRCDIR):\n    \"\"\"Create experiment folder. Append timestamp to make name unique, copy src code to folder. copy config to folder\n\n    Args:\n        path (str or pathlib.Path): name/path for experiment output\n        src (_type_, optional): location of source code that generated the model. Defaults to HOME/phd/phd-year2/src/phd_year2.\n        config (str or pathlib.Path): location of config file.\n    \"\"\"\n    if timestamp:\n        # add timestamp\n        path = append_timestamp2path(path)\n\n    # create folder\n    path = create_dirs_on_path(path)\n\n    if copysrc:\n        # copy source code\n        copy_src_to_folder(path, src=src)\n\n    if config_path is not None:\n        copyfile(config_path, path)\n    return path\n</code></pre>"},{"location":"reference/utils/#simnetpy.utils.filesys.json_numpy_obj_hook","title":"<code>json_numpy_obj_hook(dct)</code>","text":"<p>Decodes a previously encoded numpy ndarray with proper shape and dtype.</p> <p>:param dct: (dict) json encoded ndarray :return: (ndarray) if input was an encoded ndarray</p> Source code in <code>src/simnetpy/utils/filesys.py</code> <pre><code>def json_numpy_obj_hook(dct):\n    \"\"\"Decodes a previously encoded numpy ndarray with proper shape and dtype.\n\n    :param dct: (dict) json encoded ndarray\n    :return: (ndarray) if input was an encoded ndarray\n    \"\"\"\n    if isinstance(dct, dict) and '__ndarray__' in dct:\n        # data = base64.b64decode(dct['__ndarray__'])\n        data = dct['__ndarray__']\n        return np.array(data, dct['dtype']).reshape(dct['shape'])\n    return dct\n</code></pre>"},{"location":"reference/utils/#simnetpy.utils.plotting","title":"<code>plotting</code>","text":""},{"location":"reference/utils/#simnetpy.utils.sci_funcs","title":"<code>sci_funcs</code>","text":""},{"location":"reference/utils/#simnetpy.utils.sci_funcs.nanmean","title":"<code>nanmean(x, allnanvalue=np.nan, **npkwds)</code>","text":"<p>Function to compute np.nanmean and replace empty slice with  user value. Defaults to np.nan i.e. np.nanmean([np.nan,np.nan]) = np.nan.</p> <p>Parameters:</p> <ul> <li> <code>x</code>             (<code>ndarray</code>)         \u2013          <p>array to apply np.nanmean to.</p> </li> <li> <code>allnanvalue</code>             (<code>int</code>, default:                 <code>nan</code> )         \u2013          <p>Value in case of empty slice. Defaults to np.nan.</p> </li> <li> <code>**npkwds</code>         \u2013          <p>keywords for np.nanmean function. e.g. axis=0 etc.</p> </li> </ul> <p>Returns:     type: nan mean of array or allnanvalue in case of empty slice in array</p> Source code in <code>src/simnetpy/utils/sci_funcs.py</code> <pre><code>def nanmean(x, allnanvalue=np.nan, **npkwds):\n    \"\"\"Function to compute np.nanmean and replace empty slice with \n    user value. Defaults to np.nan i.e. np.nanmean([np.nan,np.nan]) = np.nan.\n\n    Args:\n        x (np.ndarray): array to apply np.nanmean to.\n        allnanvalue (int, optional): Value in case of empty slice. Defaults to np.nan.\n        **npkwds: keywords for np.nanmean function. e.g. axis=0 etc.\n    Returns:\n        _type_: nan mean of array or allnanvalue in case of empty slice in array\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        y=np.nanmean(x, **npkwds)\n        y=np.nan_to_num(y, nan=allnanvalue)  \n    return y\n</code></pre>"},{"location":"reference/utils/#simnetpy.utils.sci_funcs.non_nan_indices","title":"<code>non_nan_indices(X, offset=0)</code>","text":"<p>Find indices that do not have all nan values.   Amount of nans needed to be classified as a nan index can be adjusted with offset.</p> <p>Parameters:</p> <ul> <li> <code>X</code>             (<code>ndarray</code>)         \u2013          <p>two dimensional (Nxd) array. nan counted per row.</p> </li> <li> <code>offset</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>amount of columns with values to still be considered nan.              i.e. a row with (d - offset) values missing is nan</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>np.ndarray: 1d array of indices in range [0,N-1]</p> </li> </ul> Source code in <code>src/simnetpy/utils/sci_funcs.py</code> <pre><code>def non_nan_indices(X, offset=0):\n    \"\"\"Find indices that do not have all nan values.\n      Amount of nans needed to be classified as a nan index can be adjusted with offset.\n\n    Args:\n        X (np.ndarray): two dimensional (Nxd) array. nan counted per row.\n        offset (int): amount of columns with values to still be considered nan. \n                        i.e. a row with (d - offset) values missing is nan\n\n    Returns:\n        np.ndarray: 1d array of indices in range [0,N-1]\n    \"\"\"\n    assert len(X.shape)==2, f\"X must be 2 dimensional, not {len(X.shape)}\"\n    N, d = X.shape\n    indices = np.arange(N)\n    idx = np.isnan(X).sum(axis=1) &gt;= d - offset\n    idx = indices[~idx]\n    return idx\n</code></pre>"}]}